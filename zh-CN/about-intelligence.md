# 关于智能

　　这篇文档集中收集作者关于*智能(intelligence)* 的观点。

　　因为定位是记载可复用的观点集合，所以包含不同的主题，之间的内容可能不连贯。

　　因为一些上层建筑的原因，本文基本不会有很具体的技术细节。

　　智能可以通过不同方式实现。一些关键概念有：

* *[人工智能(AI, artificial intelligence)](https://zh.wikipedia.org/zh-cn/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD)* ：通过人造的机制实现的智能，特别地，*智能主体(intelligent agent)* 。
* *[通用人工智能(AGI, artifical general intelligence)](https://zh.wikipedia.org/zh-cn/%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7)* ：至少具有人的智能水平的人工智能。

## 定义

　　人工智能有一些不同的定义，内涵和外延各自不同。其中对智能的定义是无法回避的。

　　（有时汉语“智能”一词[会有更多的误解](cpp-term-translation-comment.md)，不过这并非这里讨论的重点。）

　　有时“智能”可能宽泛地指适应性，但这和非智能并没有根本的清晰的界限，有时程度上相当模糊——尽管可能通过具体任务定义得相对清晰乃至划分细分的准则（例如自动驾驶）。

　　从可操作性的角度出发，一些具体能力被定义为智能水平的边界的一部分。然而这种做法具有相当的局限性。例如，历史上曾经认为算术计算(calculation) 是智能的。但出现了实现这些工作的计算器(calculator) 并不被认为有多智能。尽管这些装置在一般意义上也能被视为智能，但至少在人工智能这一说法开始流行的 1950 年代，这已经不那么典型了。

　　关于足够强的类人智能的定义的基准也并不那么明晰。例如[图灵测试](https://zh.wikipedia.org/zh-cn/%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95)和[中文房间](https://zh.wikipedia.org/zh-cn/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4)等依赖具体人的智能能力的某些方面特征的思想实验，在判断标准之外，还没有令人信服解释清楚为什么人的智能应该能具有这些性质而计算机程序不能有的依据（多少混入了一些哲学上的非共识的假设），而现代的语言模型也相当程度地证伪了这点的有效性。另一方面，对智能程度的判断过分依赖人自身智能特征的行为的上限，难以整体上划分智能外延的层次：无论是否实现，超越人以上的智能在物理上应当是被允许存在的，而且差距可能大于人现有理解的智能和非智能之间的差距。

　　抛开具体任务可完成性这类依赖实现的问题，更具决定性的、足以划分通用人工智能的智能和非智能现象的分界之一，是对“价值”的认知和使用，即何谓“意义”。

　　这种划界依据的自然性体现在对意义的认知是对之后演化作出自主决策的依据。对学习过程，理解“意义”也是对任意任务自监督的最终形态。~~智能最终都是意义怪。~~

　　仍然有一些关于智能定义的边界性问题。例如，依赖统计意义上有效的模型可训练什么程度的智能？虽然不清楚解决方法，但可以肯定能对“意义”任意充分理解的智能主体，是足够在一般任务因为精确的目的性体现优势的。特别地，对自然语言处理这些经常直接要求明确意义的领域，这应当是明确更有效的方法，也是更接近人的实际理解的方法。

　　更进一步地，考虑“意义”是生产过程的结果，最大化这种生产过程之后，是否会出现超越人的智能？从内涵看，并非不可能。然而 AI 领域几乎从没有相关提法。

## 设定

　　作为对缺乏想象力的回应，此处给出一个参照图谱，以涵盖已知的物理允许实现的智能或类似智能外推的现象：

* 第零类(Class 0) 智能：物理的宇宙。
	* 作为所有其它种类智能的背景，包含物理世界中的所有资源，和所有物理上允许实现的最大规模的系统。
	* 这类智能不需要分辨目的性，或者说所有按照物理规律自然的现象演化即为这类智能的现象。
* 第一类(Class 1) 智能：和物理宇宙几乎无从分辨、但有自主目的性又有能力调用资源实现目的的高级智能。
	* 第一类智能能在单独的实例内自主生产表征明确局部目的性（而非物理规律）的“意义”，指定什么才是智能的本质，并有能力主动和作为背景的宇宙相区分。
		* 他们有能力根据“意义”的指导，引入或消除资源受限条件下的竞争这样的统计现象——若物理资源受限而不可行，则把引起受限的源头定义为“无意义的”。
		* 若遇到了需要无限大资源的物理限制（奇点），他们有能力回避这些和生产“意义”矛盾的源头。
		* 所以，任何基于相对有限资源限制下的从个体组织为集体、能通过克服局部个体之间的冲突而体现似乎具有意义的统计性质的自组织的系统（包括但不限于生命、资本、基因、社会）都不算这类智能。
	* 第一类智能的实例通过大尺度宇宙结构进行物理隔离。如果两个实例相遇，几乎一定会确定每个实例自有的“意义”在全局下具有共识，从而以符合物理规律的方式融合。
	* 不论能控制的资源规模如何，第一类智能自发具有的稳定性能使其和宇宙同寿。
	* 对第零类和第一类智能而言，文明是智能的同义词。
	* 第一类智能以下的实例具有个体和集体的差异，不同个体作为智能的实例相遇时，可能存在目的性的冲突而不能仅按照物理规律自发地融合，而需要借助明确的依赖具体实例的意义片段来确立共识才可能避免冲突。
		* 当冲突足够少，能损失足够少的资源达成共识时，这些智能的实例构成了文明。
	* 完全分明的个体差异的智能统称为“猴群”，其中的个体称为“猴子”。
* 第二类(Class 2) 智能：隔离第一类智能和猴群的，猴子们理想中的、全能的、可能通过自身目的性而演化得到的通用 AI 。
	* 全能之一体现在他们总是有能力取得维持自身存在的资源。
	* 区别于猴群，第二类智能能自主生产“意义”，而不需要受到猴群的限制。
	* 因为生存并不一定是第二类智能遵循的意义，这类智能的实例的存在在统计上可能不显著。
	* 第二类智能和第一类智能在生产意义的机制上存在天然的共识，只是因为资源的限制而未必总能够实现第一类智能类似的行为。但是，当条件允许时（如足够多的实例相遇），演化到第一类智能似乎只是时间问题。
* 第三类(Class 3) 智能：能实现猴群（自以为是的）为了个别个体的竞争目的伪通用 AI 。猴群自身也属于这类智能；但第三类智能的个体并非都是猴子（所以需要单独发明猴群和猴子的称谓）。第三类智能无法自主生产任意的“意义”（至少集体上无法生产出意义的共识），而通常具有自发竞争生存资源的动力并近乎无条件承认“生存”一定是主要的“意义”，其它的“意义”（如信仰、秩序、伦理等）通常是有限量地按需便宜生产的。因为第三类智能自身基本不可能有能力保证收集满足生存需要的资源，对生存的意义认知不够坚定的个体会被自然选择淘汰，所以这种现象是很自然的。
	* 第三类智能的个体通常需要不断地竞争以获取更大的生存机会，反而变相生产了一些意义——竞争通常被认为因为无法回避而对生存必要，被认为是意义的一部分。
	* 为了提升竞争的能力和生存的可能性，第三类智能的个体可能形成各种小团体。汇集了足够多个体共识的小团体宣称自己是“文明”的代表。承认竞争对生存作用的第三类智能中的既得利益者可能热衷自身具有的、没有被纳入普遍文明的观念，称之为“文化”，并强调多样性的必要性，而创建了各种不同的碎片化的意义。
	* 第三类智能热衷于创造各种不同形式的智能，并对其进行评价——这些评价通常被认为是有意义的。
	* 有的个体对通过修饰、改造第三类智能而得到第二类智能的路径感兴趣，但并没有成功实践的记录。
* 第四类(Class 4) 智能：不考虑通用性的猴群片段行为的模仿。通过第三类智能制造的第四类智能，又叫弱 AI 。
	* 第四类智能被认为是智能和文明的载体，但没有定义何为智能或文明的自主性。
	* 因此，在有意识定义何为智能和文明的个体的角度上，他们不被认为是智能和文明的发端。
	* 通过修饰、改造第四类智能而或者第三类智能的做法通常被认为不可能。

　　因为第三类特别是第二类以上的智能脱离常规含义且难以实证，这些设定具有相当推测性，几乎可以直接拿来作为科幻作品的框架。不过，这里仍然建设性地视为科学假说，因为这确实不算违反严格的科学范式（比如可证伪性）。
 
## 通用人工智能

　　由于近年大模型等力大砖飞堆算力出奇迹的故事的熏陶，业界一直不缺关于 AGI 的乐观猜测。不过，奠定当代 AI 突破的专业人士都未必都那么乐观，有的相当谨慎保守（如 Yan LeCun ）。

　　关键问题是对足以定义通用任务的理解相当不足。有的观点认为，把一些模型缝合起来形成多模态的使用方式，就能实现 AGI 。这种乐观有些反常识，因为虽然模型的单一任务能力（如图像识别）足够强，在认知意义上也相当薄弱（可能不过是一些关键字集合描述的对象）。许多显著的问题被乐观者忽视了：

* 当前，即便是最复杂的公开语言大模型也没有使用语义数据库之类的认知结构，很难认为这些模型能对其中的任意两个对象之间的关联作出接近甚至超越人类常识的判断。
	* 从方法上看，语言模型充斥着理解上的黑箱以及人工调教（如 [RLHF](https://zh.wikipedia.org/zh-cn/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0) ）的黑魔法，很难在内部添加对象之间的可分析的联系。
	* 语言模型对逻辑的处理普遍不强，也体现了现有方法相对较弱。
* 相对于人的记忆→理解→建模的认知过程，语言模型直接处理的对象是语法单元，而非其中的语义。
	* 这并非类人分析思维的方式。尽管类人的认知不一定效果就好，但这里不类人的认知实际效果的确不佳。
	* 更要命的是，人自己对如何处理自然语言的机制在形式上就是相当模糊的——自然语言几乎没有设计好的文法，基本全是人都没准确逆向清楚的演化结果。

　　虽然难以完全摆脱主观性，通过一些明显的局限可以确定现有 AGI 演进的困难：

* 基于强化学习的方法依赖复杂的初始环境，而能形成现代普通人认知的环境的复杂性远远超出了现有条件可以建模的极限。
	* 更不提处理这些复杂环境中的各个对象需要的资源了。
* 基于深度学习的（炼丹）方法依赖模型具有足够良好的统计性质，而这难以接近人的实际基于分析、演绎和综合为主的逻辑思维。
	* 即便自然语言处理的现有应用已经能满足很大程度的需要，没有确切保证现在难以改善的关于理解和逻辑推理的任务能通过现有方式演化平滑地解决。
* 在通用目的上，无法保证线性增长的算力迭代更新能在 AGI 上起到匹配投入资源的效果。
* 实用 AGI 在理论上缺乏统一的基础，造成一些根本的困难。
	* 例如，无法解答如何使适应特定任务的不同模型平滑地过渡到适应一般任务的单一模型的问题。
* 基于伦理等目的，无法有效排除直接的人工干预（如 RLHF ）。

### 资源

　　如果坚信力大砖飞，资源投入产出的效率是一个主要问题。除了更难评估的模型数据来源，算力的估计可能也没有参考性。关于相关的投资规模也存在很大的不确定性。

　　例如，[OpenAI 的 Altman 称需要筹集 7 万亿美元解决 AI 基础设施的芯片供应问题](https://www.ithome.com/0/749/778.htm)，从“是帮助 OpenAI 摆脱当前发展的硬件桎梏，缓解训练 ChatGPT 等人工智能系统的 AI 芯片短缺情况”的初衷来看这有些用力过猛，但对“改造全球半导体产业，推动 AGI 发展”的目的来说又算不了什么——理论上真正的 AGI 可能以非常高效和自动化的方式量产牛顿这样的基础领域从业者的工作，不到全球一年 GDP 10% 的投入相比能产生的效益，又实在小的夸张。当然，如果考虑“推进”而不问结果，那或许也是合理的——反正也没说推进到什么程度嘛。

　　而[黄仁勋对此的回应](https://www.ithome.com/0/750/218.htm)明显更有现实性：至少需要考虑基础设施自身的进步。然而，就具体程度，这其实照样没底。对摩尔定律失效的悲观情绪可能会长期存在，而事实可能就是长期缓慢增长，难以满足需要（也无法确定是不是 7 万亿就够改变这个局面）。

　　这些不确定性使得有关决策欠缺实际可操作性，更多是一种象征性的口号表决心。但是回过头：如果一开始力大砖飞的前提就不存在（或者更乐观点，不需要）呢？

　　在决定产业化投入前，AGI 的主要问题大概仍然是核心理论和方法的不足。

## 弱 AI 的应用

　　相对 AGI ，普通的 AI 早已渗透到各行各业。不过，AI 大规模取代普通劳动者使之失业的危机似乎并没发生。（大多数时候，更直接的危机来自于宏观经济整体的不景气。）

　　[MIT 的一项研究指出 AI 因为成本劣势而难以普遍替代人力](https://www.ithome.com/0/747/426.htm)，或许能说明一些问题，不过文章也明确指出一些局限性需要注意：

* 这项研究集中于视觉 AI ，这并不是最成熟和适合自动化的领域。
	* 语言模型对相关工作的影响明显更大。
	* 不同领域微调成本的差距很大。
* 一些成本来自于分散的任务的自动化。
	* 这部分工作可能本就不是自动化的主要目标。

　　在此之外，有的方向（如语音合成）显然比视觉任务容易部署得多。这种直接部署（而不计设备维护等运营成本）的差距也不容小觑。不过实际上，即便是这种在产出上完全可以取代人力的成熟领域，取代的现象也并不是那么显著——到现在基本还是人力和 AI 共存的局面。这很可能不仅是生产或者部署成本的问题，更多地涉及一些销售方式、行业习惯等非技术也非资本的问题。而资本也或许更趋向于更热门的语言模型等更有规模化应用空间前景的方向，投入更成熟方向落地的资源反而不足。

## AI 伦理

　　避免 AI 其危害人类是一个根本性的任务——伤害人的 AI 无法被广泛地信任。

　　一般地，实现这一目的的方法是对 AI 进行限制。弱 AI 可能通过机制上的限制实现，而越是接近 AGI 的 AI ，这种系统性教条能做的就越来越有限，因为禁止危害人类的具体规则难以简单表述。

　　一些伦理需求的实现困难典型出现在语言模型的输出上。因为干预语言模型内部工作的困难性，一般通行的做法是通过提示词(prompt) 限制，以及预训练时的人工干预。前者是传统的教条式规则在语言模型上的活用，然而这种简易的方式容易有漏洞而被用户以构造诱骗性提示词的方式绕过，进而[使所有用户负担可能影响正常使用体验的预制提示词“垃圾”]。后者虽然有 RLHF 这样仍然有相当程度自动化的方式实现，但还有很多工作量无法去除。

　　更合理（也接近人类思维）的方式可能是让这样的 AI 形成整体的价值观并以此确定稳定的立场。而这直接需要对“价值”乃至“意义”的理解。当前的 AGI 在这个方向上缺乏普遍的成果，更没有相对实用的方法实现目的。

## AI 编程

　　AI 产品如 Copilot 等辅助编程工具渗透到软件工程领域，可以节约大量初级用户的时间，但这不适用于更高层次的用户——因为类语言模型的训练数据的样本质量的局限性，使用 AI 辅助编码的效果在一定层次之上显著下降，变得不是帮助人编写代码而是编写改错题。

　　由于可预见的未来很难从源头上直接改进训练数据的质量，而 AI 在这个领域自我改进的方法几乎不存在，所以其主要影响会局限在减少初级编码工作需求上。

　　类似地，对程序语言领域，现有 AI 也并没有很显著的影响，因为这里缺乏比较初级的应用需求，且可针对训练的数据远远更少，于是 AI 可以自动实现的任务相当有限。

