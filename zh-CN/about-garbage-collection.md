# 关于GC的意见

Created @ 2014-07-18, rev2 @ 2014-07-20.

## 原始论述（“盖棺定论”）

### 0

这里的GC是指“垃圾回收”(garbage collection) 机制，不是指其它有的没的——比如《罪恶王冠》。

也有人（如裘宗燕）[指出“垃圾”不是适当的译法而照习惯作“废料收集”](cpp-term-translation-comment)。不过现在大多数用户看来已经习惯“垃圾”了。

GC 不是没用，不过早就被滥用了。这篇文章主要是婊无脑GC厨的——特别是连GC干嘛都没搞清楚的大多数小白。

GC 的实现技术细节不在此盖棺定论范围内。

### 1

虽然语义上并没有限制，现实的GC收集的是动态存储空间，通称内存。无论收集的是什么，首先一定是资源。

遗憾的是漠视资源语义的渣用户太多了，导致撸的程序运行时性能经常惨不忍睹。这是GC招黑的一个重要原因——倒不是GC本身的概念或者实现烂的关系。
既然是资源，一个现实特性就是有限性。在这里存储资源的有限也就是认为计算机实际上能具有无限的内存。以这种臆想为前提设计程序，显然是幼稚可笑的。

### 2

GC和非GC相比，主要特点是回收资源时机的非确定性(indetermination) 。

这点首先对管理的资源特性有要求——即便延迟释放也无所谓。而其它资源就明显没法使用GC。

要是真能有效管理资源，那么延迟也是合理的。毕竟“不用白不用”。不过现实呢？

很不幸，貌似很多而释放起来又欠缺对环境造成副作用“内存”作为了首要的冤大头——也被很多小白作为“不用白不用”的典型了。

可惜现实是很多用户内存根本不够用——特别是机器上的物理内存。一旦用量接近上限，频繁触发操作系统的换页机制，体验就立马下降几个档次甚至根本就因为失去响应而无法使用。

作为知道发生了什么的最终用户，我实在对一个程序不停地把可用资源变为垃圾同时因为不断地的“收集”做白工吃CPU和磁盘I/O忍无可忍。

我就想说，你丫的能把程序写得不那么蠢吗？！——有时候还真不能，这个等会再说。
（讽刺的是，这种情况似乎在服务器上——特别是Web服务器上，更能被容忍和接受。土豪傻多速？行业惯例？）

### 3

GC根据实现基础原理可以分为两类，一个基于引用计数(reference counting) ，另一个基于追踪(tracing) 。原型都在1960年被发明，不是什么新的东西。

基础原理区别很明显。引用计数其实不只适用于延迟释放的情形，也适合其它资源管理，它本身不依赖于非确定性释放；但是相对地，它没法照顾到“循环引用(cycle reference) ”（循环引用究竟是什么玩意儿暂且不表）。

相对地，追踪依赖于允许延迟释放，但能处理循环引用。

应该说有效性上两者都是明显的，而侧重不同。不过现时典型的GC实现都会照顾循环引用的情形。可能正是因为这样有不少人有“因为原始的引用计数没法处理循环引用所以不算GC”的误会。
实际上还是有不少GC是基于引用计数的。不过它们不约而同地放弃了对不依赖延迟释放资源的管理能力。这才是作为GC的原因。纯粹的引用计数实现的资源管理不被作为GC并不是因为欠缺循环引用。

引用计数的另一个缺点是簿记和修改引用计数会引起额外开销。这个缺点也不限于GC范畴。在此先略过。

### 4

所谓循环引用，字面上就是是指引用的“循环”。为了在一般性上说清楚，先得在脱离具体语言语义的情形下，说明这里的“引用”是什么——很简单，是指资源实例之间的一种被动的“从属”，或者说“依赖”。

为了便于使用，清晰的“引用”需要自然满足两个性质：反对等性——如果A和B之间存在引用，要么A引用B要么B引用A，两者互斥；以及能够间接引用：若A引用B，B引用C，则A引用C。

形式地说，理想情况下的“引用”是指两个资源实例之间的一种反对称的传递的非空二元关系。因为满足传递且不满足对称，自然是反自反的。

而循环引用就是指以资源实例为顶点的（有向）关系图上出现了环，和上面的定义相矛盾：也就破坏了这些容易理解的、清晰的性质。

可以推定这些性质和资源管理有直接影响，原因是这里资源引用的逆关系——资源所有权(ownership) ，和资源管理的操作密切相关。

显然计算机系统中的资源并不是凭空产生的。可见的资源的可用性必然通过系统中的其它组成部分授予。这种事实蕴含了和上面清晰的“引用”一一对应的性质。

举例说明。在典型的宿主环境下，程序使用的内存由系统从虚存中分配；而虚存建立在物理存储之上；反过来，硬要说物理存储依赖虚存，而虚存依赖于程序使用的内存，则根本是无稽之谈。
大到计算机系统是如此，小到程序内部也是一样。内存分配器完全可以继续照搬这种明确所有权和依赖关系的抽象。

这里的一个重要结论是：合理的抽象不可能导致“循环所有权”的问题。作为逆关系，“循环引用”也理所应当不应该存在。

### 5

那么为什么会有些人就会纠结“循环引用”呢？

很简单，因为那些人（很可能是稀里糊涂地）选择了和自然的资源管理抽象相矛盾的使用手法，导致他们眼中的引用没法作为所有权的逆关系。

进一步的理由是他们目无资源管理，也没有资源所有权的概念。

于是又回到起点了——蠢不能怪别人，是吧。

当然，有人会说，递归数据结构就是应该能自然存在的。

没错，这是很自然。但这种自然没强力到让实现掩盖物理规律的程度。

所以，预设使用GC思维的Lisp机得在内部消化掉这种矛盾。相对地，其它的“肮脏”的实现则把问题抛给了上层。尽管事实上是搅得一团糟，但这种方式却生存了下来。

毕竟用户自由还是别拿来忽悠的好。

### 6

引用计数打破循环引用的思路很直白清晰：区分出什么不是“真正的”引用——也就是所谓的弱引用(weak reference) 。剩下的“真正的”引用，相对就叫强引用(strong reference) 了。

强引用符合上面所有的“自然”的资源抽象的性质。因此，基于这种抽象的std::shared_ptr和C++的RAII一起也工作得很好。而std::weak_ptr就是二等公民了。

在GC实现中可能会有更复杂的分类以适应不同的收集策略，语言也可能提供更多的接口以利用这些设施。比如Java在java.lang.ref中提供了SoftReference<T>、WeakReference<T>和PhantomReference<T>等。

具体这些接口是否在一般情况下有必要暂且不论，但可以确定：GC已经把问题搞复杂了。更糟的是，作为公开语言实现的接口，用户要么无视这方面的问题，否则即便全是自己的代码也迟早绕不过这些复杂性（不像C++里要是觉得线程安全不爽等等就可以无视std::shared_ptr自己搞个山寨货出来）。

追踪实现则使用了一种整体上“启发式”的做法同时顺带避免了循环引用——这是往好听来说的。具体问题看来更多，先放置play。

### 7

话说回来，有人可能会问，我就是需要一个图(graph) 怎么办？
答案也很简单：把具有所有权的资源实例放到外部。
不要在这里纠结于去中心化的无聊洁癖。资源不可能是凭空从系统内部蹦出来的，自然的资源抽象本来就没有这个性质。

唯一能摆脱这种约束的手法就是在现有系统外部找到其它可以依赖的东西。
其实说白了，GC还不就是这种外部资源所有者嘛。只不过语言支持的GC往往习惯于设计成刻意唯一的“权威”所有者罢了。

在资源管理上，这和实现这些语言的元语言（如C和C++）中用户自己维护的内存池等玩意儿本质上又有多少区别？

——就是用户管理具体资源实例的能力被阉割了一大半。仅此而已。

### 8

具体阉割了什么部分，其实具体实例按场景来分还是有好几种的。

分类一：在这些使用GC作为默认资源管理手段（这里不说内存管理，是因为反正其它资源没法靠这个管得了，所以两个就一样了）的语言中，用户不可能实现一个和默认GC对等的资源管理手段。

实现这样的GC，只能分隔实现运行时环境。虽然有些语言在这里也可以自举，但毕竟已经低人一等：你只能提供实现，还是得用语言提供的接口。也就是继续被阉。
再如，如果一个GC基于引用计数，用户觉得不爽，也没法要求改变机制。因为GC没有暴露底层接口，在这类语言中就只能靠改动语言实现（运行时）去擦屁股了——否则只能等语言设计者和其他实现者开窍。这个例子也不用我多举了。

分类二：你没法干预对每个单独资源实例的管理。

这个可能更具有现实意义，因为矛盾更常见点。

比如，如果一个GC基于引用计数，用户觉得——还算可以忍受，但就是不喜欢个别资源上被引用计数因为他知道这个资源只会引用一次，怎么办？

如果真只有GC，一般答案就是凉拌了。

反之，不纠结GC，可能有其它解决方案。

先举个黑boost的例子。就是那篇转了几次的《shared_ptr四宗罪》里说的修改引用计数的性能开销。（虽然不和GC直接相关但是和这里的场景是一致的。）

不管是boost::shared_ptr还是std::shared_ptr，这文章里说的问题是的确存在的。

那么真正的问题呢？解法呢？

很简单：这里根本就不该使用引用计数。“标准”解法是，如果知道不需要重复引用，使用std::unique_ptr保持所有权，然后使用其它指针（很遗憾，经常是内建指针）来作为“弱引用”。

### 9

“唯一”还造成了其它一些困扰。比如说多线程中，即便用户知道运行的上下文，也不得不同步，而不像普通的池一样，只要资源允许，想有几个实例就几个实例。在这上面的性能问题不胜枚举（引用计数看来更加倒霉一些），且略。

引用《对比Ruby和Python的垃圾回收》里的说法， GC 是应用的“心脏”。看来是不错，要害实在太明显了，实现一烂整个完蛋——而且没法补救。

资源管理应该更像是淋巴才对。

### 10

关于追踪 GC 的实现。

启发式策略实际上呢……说白了，很多时候是瞎猜。

比如什么weak generational hypothesis？——好吧，某些样本的统计上也许真符合煞有其事。但这丝毫不能掩盖指望让用户“按常理出牌”而剥夺用户对资源控制的自由的事实。

顺便，关于分代GC，设计者恐怕也未必能讲清楚，具体几个代在什么场景下是最好的。搞不好实际profiling都很难设计用例。

这种人为附加的系统性分析困难有那么值钱么。（说不定这能解释难怪有那么多VM“调优”蹭钱的了？）

而用户实际上被坑的更多的是……为了回收资源这么些破事就得stop the world（PAD长即视感……），放到客户端一旦能体验出来就是个笑话。

别说相对于引用计数总的开销有优势这些胡话，引用计数就是再不能忍，好歹也能平摊时间复杂度，在时间利用率和响应上没这么搞笑。

此外整体上，GC，尤其是追踪实现的GC，内存利用率令人发指。《Why mobile web apps are slow》指出要想流畅利用GC，需要准备至少5x的内存空间。
因为本性难移，没法指望GC实现技术的飞跃发展，所以至少在移动平台上，稍微“大型”的应用（游戏？），短期靠依赖GC的语言作为主要实现，已经被毙了。

老实说，我认为只要上面为例的这样成吨的冗余困难不被解决，GC就不适合放到严肃的系统语言里面作为默认配置。

### 11

强迫用户使用GC更是显然的愚蠢。

没错，噗的首先就是Java。

C#之流就是山寨也好歹能提供点绕过GC缺陷的方法来补救不适应性。Java……呵呵。

当然，GC不是Java最蠢的地方。比如说培养目光短浅的蠢货的有效性上来说就不能全怪GC（但是不少Java用户“目无资源管理”“异常安全没概念”这帽子应该没扣错）。

不过这个是题外话。异常安全什么的也不跑题了。

### 12

为什么很多人会使用GC？

很多小白用户的第一反应是：为了不泄露。

的确泄漏是很不爽的蠢事。……不过它们往往漏了主语。

资源泄漏。不只是内存泄漏。

内存是资源没错，别忘了套接字、数据库连接……这些没亲爹GC罩着的资源。

没有清醒的资源管理概念，照样总有一天被坑。

现在Java稍微也认清了点形势，提供了try-with-resource的糖来救场。效果嘛……先不论，教会大多数用户使用再说吧。

由于不提供确定性析构，像Java这样的语言中提供了终结器(finalizer) 的概念来擦屁股。
可惜根本擦不干净。

Java的finalizer和C++的destructor不同，JVM保证一个死对象调用一次，但用户能当作普通方法来调用。

容许这种逻辑与其说是灵活，不如说是满足稀里糊涂瞎折腾的需求。

为什么在语言的层面上允许死了又活的对象？

### 13

以前为什么会使用GC？

历史上先扯开GC大旗的是Lisp（方言）。

本质上来说， Lisp 提供了对于当年的条件来说过于高层（完整实现起来过于困难）的抽象。
J.McCarthy 提出的 GC 以当时来看固然是一种不错的发明，但只能作为一种一揽子实现以避免在一种强调符号操作的语言中夹杂不彻底资源抽象接口的“变通”。
即便是对现代意义上的 Lisp ，纯 GC 也未必是适合偷懒的方案。虽然种种原因导致 Lisp 实现几乎总是要背着一坨很厚的运行时，多少掩盖了这点（其它不得不需要 VM 的语言也一样）。

### 14

黑了那么久的GC，GC真的一无是处了吗？

这点我要承认，倒也不是。

GC通常对缓存更加友好。比起单纯的引用计数，GC没有持续的和应用逻辑无关的冗余的存储访问。

这样，和J.McCarthy等的初衷不同，现代GC的本质上能体现的积极作用是以多余的存储和响应为代价，换来吞吐量。这也可以解释在服务器上使用GC的实现更能被接受。
另外一点，如H.Sutter指出，GC可以避免一些并发访问的破事如ABA问题，这能简化一些并发操作的实现。

不过并发程序设计上有更本质的一些近乎哲学的设计策略问题，比如说优先避免共享数据（如Erlang）还是优先避免数据可变（纯函数）……比GC的自个儿的问题还麻烦了。
除了上面可怜的几点之外，GC几乎还真是一无是处……（别把纵容愚蠢粉饰成优点就是了。）
那是现实。

如果说要进一步避免这些缺点，当然还是有很大空间的。这里不说实现细节，光说体系结构的方向。

——就是尽量往底层。没错，往底层发展。

做成硬件嘛消费者买不买账我就不管了……不过至少在宿主实现（操作系统内核）上还是有些可能性的。

比如，如JVM或者CLI这样的应用虚拟机在多进程上的存储浪费，操作系统得负担一部分责任。

如果GC是系统提供的，那么垃圾相对冗余容易少一点。而控制存储占用的接口也能做得更加灵活。尤其是调度优化方面近水楼台。

用户空间搞这个看来不太容易也没多少效果，所以到底得扔给内核。

一句话：就算仍然是垃圾，比应用层面上的垃圾危害小。

不过搞内核的嘛，看来大多也不会乐于往里面塞垃圾就是了……所以可能还是得靠拿这些具体VM做内核的来折腾。短期也不会实用。

于是就暂时到此为止了。

## 引用回复

（为避免原始内容失效，复制回复内容视为合理使用在此引用。）

### 2015-03-07 10:55 cqwrteur

棒棒哒！最近写wow插件，看着插件内存统计才深刻感觉GC不光在泄漏其它资源，即使是在管理"本职"的内存资源上也是渣。有些人写了一个破插件实际上只占用2mb内存，却十分快速的从2mb内存占用提高到20mb以上，然后又执行了gc内存占用率重回2mb,以此类推。实际上一执行gc就游戏卡的不行，确实是world stop的既视感。

### 2015-03-07 10:55 cqwrteur

妈蛋一个破GC会让内存多耗10倍还来回分配释放谁都不爽。

### 2015-03-07 17:48 幻の上帝

还要补一点：把逼别人读依赖GC的烂代码正当化。

### 2015-04-06 16:06 Tippisum

#### 1. 关于引用计数的线程同步开销（以及其他性能问题）。

如果一个资源必须要涉及到在并发情况下的正确获取和释放（无论因为任何原因 e.g. 这是一个内核对象或者外部对象），那么这个复杂性是本质的，不会因为使用手工管理/RAII/引用计数/GC等任何具体的手段而改变。

当然这种模型自身是不是有问题另说。如楼主所言，一个好的资源管理模型应该要避免这种混乱的状况。但现实世界的需求总是充满着各种乱七八糟，这也不容否认。

如果资源所有权的获取，转移和释放不涉及并发过程，也不涉及跨模块互操作，那么理论而言这个东西是可以优化的，不用每次都耿直的去AddRef()和Release()——原则上并没有什么理由禁止编译器优化掉所有编译期可以确定的引用计数操作。而且很明显，绝大多数资源的分配，或者说正常的资源分配都应当属于这一类，否则RAII也一样没得用。认真来说，在充分利用编译期可以确定的信息之后，引用计数是有可能优化到和std::unique_ptr一样的开销的。

为什么实际上做不到？一个重要的矛盾就是，只要上面说的跨线程/跨模块问题还存在，引用计数就必须是线程安全的。而这极大的限制了可能的优化。C++，或者说现在的任何一种语言都不能表达“虽然引用计数必须是线程安全的，但任何时候同一个线程内出现AddRef() + Release()它还是总可以等同于一个no-op”这样一种约束。除非提供语言级别的特殊支持，否则指望编译器做这个优化显然不现实。

#### 2. 关于Finalizer。

说实话确实很难理解Finalizer的各种奇葩行为。尝试梳理一下。

之所以我们会需要Finalizer这种东西存在，那是因为有GC无法管理的资源。换句话说，Finalizer唯一且最大的意义就是提供一个回调，用于实现自定义的清理逻辑。
从低层次的技术角度上讲，因为程序员可能会干奇葩的事情，比如说在Finalizer里把this指针注册到一个全局对象上面去，所以一个对象可以死了又活。但问题在于，为什么要干这种事情，或者说，设计上为什么要允许？

在我看来，只有一种有效的解释：因为Finalizer有机会涉及到一个可能耗时很长的异步操作（e.g. 关闭一个远程数据库连接），而这个操作需要保留当前对象的状态，所以需要在Finalizer之后依然让这个对象再“多活一会儿”。
但事实上这个复杂性完全不是本质的，可以有其他的解决办法。选择这样的设计几乎可以肯定是偷懒。而这样的偷懒，毫无疑问让整个资源管理机制变得混乱并且降低了效率。

#### 3. GC的限度。

任何GC必然都有限度。用户态的内存无疑是好管理的，区别只是在于具体的策略。相比之下，内核对象就没那么好管理了——麻烦的是这玩意儿偏偏还就是很多程序的命门。所以说对于内核态GC的需求是完全可以理解的。但不管怎么说，GC的管理范围还是有限度的。即使用上了内核态GC，从此FileStream也能自动管理再也不用Close了，用到外部资源（如数据库连接）的场合又该怎么说？总不能跑到别的服务器上去GC吧。所以说GC的可扩展性是必须的。最常见的手段是Finalizer，但已经有不少奇葩的设计搞得Finalizer不再像是一种正儿八经的扩展机制，更像是亡羊补牢的手段——如果一个对象声明了Finalizer，就在GC里为它应用特殊的回收策略，保证尽可能快的调用Finalizer，这真的是一种很难想到或者很难实现的东西么？总觉得事情不应该是这样子的。

### 2015-04-06 17:27 cqwrteur

我并不同意内存用GC好管理的说法。魔兽世界插件就是个典型的例子。实际上这类做法经常使得内存的分配量和CPU占用率直接挂钩。由于频繁GC，插件经常造成游戏卡顿，为了处理这类问题，程序员不得不用诸多的TRICKS去避免内存重复分配以至于大量GC的情况，无疑使得代码更加混乱。

### 2015-04-06 17:31 cqwrteur

魔兽世界有一些类似Weakauras的插件，他们就受这类问题的影响。经常内存频繁分配释放。本来只占用2MB内存的程序，由于GC的原因占用到了10倍内存也就是20MB，然后又被释放回到2MB，如此频繁的分配释放显然使得用户不爽。

### 2015-04-06 17:35 cqwrteur

我想Finalizer根本解决不了问题。例如某程序不怎么使用内存却需要使用文件，我不信GC有什么有效的算法去调用文件关闭。

### 2015-04-06 17:50 Tippisum

回复 cqwrteur: 好吧，澄清一下，我说的“好管理”指的是“容易将其纳入GC的管理范畴”（参考上下文“内核对象没那么好管理”），不是指“容易开发出高效率的GC实现”。目前大部分的GC实现，尤其是那些自己撸出来的GC实现，有各种性能问题实在是再正常不过，更别提能够适应应用场景选择正确策略了。

### 2015-04-06 17:54 Tippisum

回复 cqwrteur: 如果不是GC有各种各样的问题（而且其中某些属于GC的资源管理模式造成的难以解决的困难），那它早就一统天下，也就不会有这篇文章存在了。毕竟不论资源利用的效率问题，单就方便性而言，GC毫无疑问是个重大的进步。

### 2015-04-06 18:06 Tippisum

回复 cqwrteur: 另外，楼主的文章里默认GC都是non-deterministic GC，也就是没有确定时间析构的。但理论上并没有阻止deterministic GC，也就是可以提供确定时间析构的回收机制存在——尽管由于各种原因，它们似乎并不是当前GC领域研究的重点，但认为GC不可能被有效的扩展我认为这个论断过于绝对。

### 2015-04-06 18:08 cqwrteur

回复 Tippisum : 我不觉得GC真的是进步了。因为GC的设计者把世界考虑的过于简单了。

### 2015-04-06 18:18 Tippisum

回复 cqwrteur :我说了，“单就方便性而言”。假如说某一天真的发明了某种黑科技GC，能保证确定析构还不会增加任何额外开销，你说你用还是不用？这个道理肯定不用多说的嘛。当然，这种东西能不能做出来就另说了。但无论如何，GC在性能开销和资源利用上的问题，不能掩盖它在方便使用上的优点。

### 2015-04-06 19:09 cqwrteur

回复 Tippisum :我想， 不可能不存在代价的。收集资源一定是要以损失能量为代价的。不可能你不使用能量，垃圾能扔进垃圾桶的

### 2015-04-06 20:46 Tippisum

回复 cqwrteur :纠结这个问题没有意思。我的“假如”建议你按照虚拟语气去理解（虽然中文里没有这样的语法结构），它仅仅是用来假设一个理想情况，从而突出问题的重点。

### 2015-04-06 20:48 Tippisum

回复 cqwrteur :实际的应用中当然不可能没有代价，但实际的应用中你也会接受开发效率和运行时性能的合理妥协——否则的话为什么我们要开发各种“高级语言”呢，你说对不 对。所以问题的关键并不在于GC到底会不会有开销，而是GC的开销能不能——或者说理论上有没有可能降低到一个可以达成妥协的程度。 

### 2015-04-06 22:27 cqwrteur

回复 Tippisum : 我是无法理解，为何一定要陷在GC的思维里不能自拔。为何不能干脆扔掉GC，反而问题有助于解决

### 2015-04-06 22:29 cqwrteur

回复 Tippisum : 我认为GC已无需批判，被扔进历史的垃圾堆里了 

### 2015-04-06 23:00 Tippisum

我觉得楼主发这篇文章来就是讨论GC的啊，就事论事有何不可？至于什么“陷在GC思维”，我更是觉得无法get point。

### 2015-04-06 23:01 Tippisum

回复 cqwrteur :内存管理这种脏活累活，就应该要交给计算机去做，问题只是在于怎样才让计算机能够做得更好，以及在现实情况暂时还没办法完全满足需要（还没有一个足够强的AI能够了解程序员的想法）的情况下，如何针对不同的使用场景选择合适的策略。这样的想法你觉得不对么？那么你是怎么考虑的呢？ 

### 2015-04-07 19:43 幻の上帝

1.deterministic GC不在上文讨论范围之内；2.脏活累活总得干，但发明出强AI之前启发式策略基本上就做不好，那么就得让人来负责。其它见最后的回复。 

## 补充讨论 2015-04-07 00:39 Tippisum

借地讨论一些可能有点跑题的东西

#### 1. 未来的发展方向。

未来这种东西谁也不敢拍胸脯说一定会如何如何。但根据资料和逻辑做出一些合理的推测还是可以的。

* 如果要我发表看法的话，那就是自动资源管理替代手动资源管理，标准化、普适性的资源管理替代自制的、不兼容的资源管理是未来的方向。计算机科学大的方向从来都是如此，资源管理这个课题不太可能成为例外。 *

当然，到了那个时候，也许计算机已经有智能了，自己能给自己写程序，就没程序员什么事情了。这样的可能性姑且不提。不过话又说回来，在绝大多数的应用场景里，资源管理真不是一个特别深奥的问题。RAII这种东西它的灵活性很高么？作为最简单的一种资源管理模式，大部分的时候不也都够用了。

总之，未来发展的目标一定是向着更有效的自动资源管理的方向，我对此很有信心。至于这种资源管理的模式究竟长啥样，叫什么名字，那我就说不准了。


#### 2. C++为什么没有GC

GC不是一定要延迟回收的。GC也不是一定和其他的资源管理不可并存。（尽管实际使用中这两者都很常见，但有必要认识到这些都不是必然如此的）
那为什么C++这样一个非常喜欢往里面塞各种feature的语言却没有GC？

这个问题可以有很多答案。不过在我看来，其中一个重要的原因就是C++那翔一样的互操作性，尤其是跨模块、跨编译器、跨语言的互操作性。异常边界和shared_ptr之类的东西已经够闹心，再搞出一堆对某个特定版本运行库的垃圾回收机制的依赖，这还怎么愉快的玩耍了。在C++里这个问题恐怕很难有个有效的解决，语言层面上支持GC自然也就阻碍重重。我甚至觉得像我上一楼里说的那样，给引用计数多来点特殊优化，可能反倒是比较务实的思路。

再加上另一个问题，C++里面没有靠谱的元数据机制，导致运行时根本不可能准确判断一个对象的具体布局。所以虽然有各种非侵入式的第三方GC库，实际用起来问题还是很大。（因为它们对引用的判断多半只能靠猜）
语言层面上不好直接支持，第三方库又没办法写出靠谱的实现，自然导致GC很难在C++上面得到运用。


#### 3. 为什么新型语言更偏爱使用GC

a) 易用性的考虑。GC的易用性是毋庸置疑的。而且不难想象，在如今的年代，开发一个需要程序员手动管理内存的语言，不太可能会有什么市场。最简单的道理，连个内置字符串类型都没有的语言，谁会去用啊——但是事实上，正确的处理字符串操作可没有那么简单，其中就涉及到内存管理。

b) 安全性的考虑。很多新型语言被设计在受限的环境下运行。而允许程序访问“裸”资源，无论是内存还是其他资源，都存在着潜在的安全风险。有的时候，阉割用户自行管理资源的能力，也是一种feature，而且可能还是必要的feature。

### 2015-04-07 19:43 幻の上帝

@Tippisum .


其实现实的最大问题在于放任用户在不合适的时机使用不合理的资源管理手段造成习惯性生产出用户（不管是最终用户还是维护人员）体验低下的软件。至于GC只是其中比较突出的一点，所以值得单列。至于多线程和跨模块等等具体环境问题，相对于把资源管理的一般机制选择对这个问题来说，是实现细节。

有些没提到的，补充一下吧。

#### 1.GC的外延

这里讨论的GC广义上包含要求放弃deterministic destruction保证的资源管理。一般地，这几乎只有随机在线存储这种可能明显富余且不释放产生副作用改变可观察行为的资源才能容忍，所以可以说GC“只管内存”。

之前也说了，GC适合某些特定的场合，更一般情况下作为普遍手段或者fallback都是不合适的。

很明显，放弃determinstic是一种优化（避免destructor热情求值的开销），对作为一个fallback来说，这是premature optimization——不做额外的假设是没法知道这里减小的开销的代价——有没有对响应和其它行为改变的消极影响等等，是否应该可以忍受。

#### 2.引用计数

一般意义的引用计数其实也类似，只不过因为不像GC那样严格地放弃deterministic destruction所以没有必要让finalizer擦屁股的麻烦罢了。

引用计数当然也能用来实现GC，不过这需要附加上面提到的更多的限制。

要是怕分析起来麻烦，这里的基本策略还是一样：能不用GC就不用GC；同理，能不用引用计数就不用引用计数。

#### 3.语言实现的优化

很明显，C++这样的语言确实欠缺一些内建的机制使用户能指定哪些资源在运行前完全确定。但是，其它一些语言中可以有这样的特性，通过静态分析尽量提前确定释放的时机。

这种技术有时被称为“静态GC”。应当指出的是，这里所谓的GC只是类比“放置不管”的用法，实际上并不是上面所说的GC。典型地，每个资源的destruction都是确定的；即便放宽限制，也不可能完全indeterminstic，这跟静态分析矛盾。

与其说这类技术类似通常的GC，还不如说更接近RAII（好歹后者析构的相对顺序是静态确定的）。

#### 4.关于finalizer

和destructor类似，finalizer本质上也是一种“事件”——区别在于后者运行的时机通常不得不放到运行时才能决定。

但是，设计上的出发点应该是相当不同的。前者是保证deterministic以便用户表达“控制”以及适合更广意义的资源，后者则是“兼容GC”的思路下擦屁股。允许“死了又活”正体现出设计者对于这里需求把握的不确切。

如果取消GC要求放弃deterministic destruction的限制，那么finalizer也就自然退化成了destructor。

“需要保留对象的状态”和这并没有直接的关系，也不会让finalizer成为必要。

同步情形自然容易理解：destructor可以阻塞并运行很长一段时间，典型地，如析构一个std::thread之前join。

而造成异步情况比较复杂的关键的一点是过少的不恰当的抽象：映射到运行时程序存储上的资源访问状态（比如“数据库连接”“文件流”“线程控制块”），以及和程序外部环境对应的生存期不能直接由程序决定的资源位置的标识（比如URL、数据库实例和线程）其实是两回事。同步情况下假定状态一一对应相对简单，异步就暴露问题了。

换句话说，正确的解法其实是增加合理的抽象，而不是放任“复用”同一个概念，结果不得不“延长”资源生存期，使之看起来符合固有印象。

#### 5.关于内核GC

这个设想的出发点是“什么情况最能发挥GC的长处”。基本场景仍然是减少释放资源的开销以及并发。对于前者，GC实现可确定的运行时环境信息越多就越有利。
实际上到了这个程度也不是一般意义上的通用GC了，而是内核对象池。
当然，和上面一样，还是被资源自身的性质限制。像网络连接这种释放时会影响可观察行为的资源自然是没法随便GC的。但是像内核线程或者GDI对象这样的资源，还是有余地。
这里的实用并不该成问题。不过，内存页面池的使用还并不普遍，相反使用应用虚拟机却泛滥了，这有本末倒置之嫌，所以我特别提出这点。

#### 6.现状和发展

不算怎么应用，光说资源管理本身的实现技术上，大方向无非两个：在特定特定环境下更优化的GC实现以及确定阶段（比如说，源码）上的静态分析。

更严重的问题，包括标准化，还是在于人。

现状是，大部分用户连GC适合干什么和不适合干什么都一问三不知，却被默认使用GC的策略绑架了，根本就没什么机会细心思考怎么管理资源。

他们被灌输重要的是“解决问题”的抽象和具体手段比如算法和算法的实现，却不知道资源管理是贯穿“使用计算机”这个领域的核心问题，无论怎么都无法回避，用不合理的通用策略就是推卸责任，让解决问题的效果更差，而并不会让这部分自然失踪。

注意这里的效果包括两个方面。一方面是程序运行的性能可能不理想，这点前面提了；另一方面是表达意图的模糊。

后者和GC本身也没有直接关系，甚至糟糕的手动显式资源管理也能有类似的效果（一个例子是我坑了若干年的Auckland Layout Model的C++实现，里面线性规划的手动new/delete直接看根本不知道在干啥），但是影响恶劣到值得作为GC的缺点之一。

实际上，当我去阅读这样的代码，除非有更清楚的文档解释，就被迫去当人肉GC——根据上下文去猜什么地方该填充释放或者释放的资源之间的所有权。这是非常糟糕的体验，即便以对资源管理的认识不足为由需要一定的妥协，放任这种可维护性极差的代码也是不可理喻的。

所以说，未来要发展，最大的问题恐怕还是教育。特别是应该教会正视现实问题。

#### 7.互操作性

这确实是非常闹心的玩意儿。语言在这里毫无疑问有一定的责任（例如C的对象语义和布局耦合），但更大的问题恐怕还是在于人——作为专业用户，却错误地依赖本来不保证靠谱的东西（通常的ABI都算）。

我不认为C++能解决这里的问题。比如说布局，虽然它已经通过non-standard-layout允许实现自由选择而提醒用户不要依赖这种不靠谱的玩意儿，却没有提供可移植的更明确的操作方法。

这个问题并不限于GC，而且本质上恐怕比资源管理更麻烦，因为这里依赖的历史包袱不是通过提升教育水平和生产力看着不爽的东西重写替代就能了事的。就是C和C++完全退出历史舞台也不太可能解决（虽然可以变通）。

#### 8.易用性

除了不重视资源管理的教学导致的“易用性”错觉，语言设计者自己也有责任。
GC一开始是John McCarthy发明的，他某种程度一厢情愿地认为这是通用解决方案。
遗憾的是虽然AI winter击碎了不符合现实的幻想，这一块却被James Gosling等捡起来了。
我不完全清楚他们当时的动机，但是基于对C++复杂的叛逆心理，“易用性”显然是重要的一点。
现实仍然慢慢瓦解这些人的幻想，可惜就是不够快，结果“业界”充斥这一大堆垃圾。
不管怎么说，出来混就是要还的。
要是C++不幸会有这种麻烦的错觉，那么估计也会有脑袋清醒的划清界限发明在其它方面都能取而代之的语言。现实的C++没有这种强迫症，自动资源管理也几乎总是更清晰，很好。

#### 9.安全性

像Java、C#这样增加运行时中间层的做法，在预测确定的程序行为（基本避免未定义行为）上的确有一些帮助，不过代价就是特性缺失。

操作更底层实现的功能就是features。虽然我不觉得底层的功能就应该内建，但是通过抽象使之不得不用完全不同的做法来实现不同层次的功能，这是对语言抽象能力的损害。

作为妥协，这种抽象的损失在DSL或许可以接受，但作为通用目的的语言来说，这种artifact是不合理的。

C/C++这里其实也并不怎么样，脱离了__扩展还是很多残废。不过，好歹没有发明不同的“native”之类的东西把问题更复杂化。

内建GC是一种乍看起来有效的策略，但另一方面，也就是体现对“资源”抽象能力弱，而不得不特
殊化特定资源的抽象能力缺陷罢了。

这里所谓的安全，基本也就是流于实现的马后炮而已。

### 2015-04-07 19:41 幻の上帝

但是通过抽象使之不得不用完全不同的做法→但是通过抽象实现使用的中间层使语言的用户不得不用完全不同的做法

### 2015-04-08 01:41 Tippisum

@幻の上帝

#### 1. 不讨论deterministic GC，那就是说把GC限定在使用延迟释放来作为优化。当然这没什么问题，本来也没多少人搞deterministic GC（至少现在没有，以后会怎样不好说），所以不提也罢。

我本人对于non-deterministic GC，或者至少是策略无法控制的non-deterministic GC总体上还是倾向于保留态度的。道理自然大家都懂，只有资源比CPU周期更不值钱的假设成立，non-deterministic GC从设计上才有存在的意义。所以现在不说GC到底“能管”多少，反正勉强算是适合GC（此处往后如无特殊说明均不再包含deterministic GC）管理的资源就只有内存了。

不管三七二十一首先上GC，从优化的角度看，毫无疑问就是你说的premature optimization。

#### 2. 策略选取

假设资源是紧张的，手工管理一切当然效率最高。再往上，RAII的开销相对小（但不是零。C++11的右值引用对RAII的是个质变的提升，说实话在这之前就咬定说RAII一定高效率的还真是有点让人不能苟同），引用计数因为更难优化的关系开销更高些，GC就纯粹是往另一条路上走了。
事实上对于资源使用的静态分析一直是我思考的一个问题。因为实际应用中大部分的资源，尤其是那些紧张的资源，它们的使用模式很多时候都是固定的——或者说这正是RAII存在的理由。问题在于C++本身仍然不能很好的表达资源管理的语义。RAII，至少在它最初被提出来的时候，更像是一个trick，而不是有着良好设计的资源管理。

C++都已经是这样了，指望其他的语言能提供更合理的静态分析，我觉得可能性很低。何况现在GC这么火，愿意沉下心来干这一套的人恐怕更少了。

#### 3. Finalizer问题的补遗

理论上说同步情形下Finalizer基本等同于是执行时间不定的destructor，是没有什么保存状态的需求的——无论如何，对象的状态至少会保存到Finalizer返回，而这个时候所有的清理逻辑都应当已经完成了。在这种情况下，除了发神经，我无法想象任何让对象死了又活的必要。

所以唯一的问题在于需要异步执行的清理操作，而且这个操作还需要访问正在被清理的对象的状态。异步操作模型的设计这是另一个问题了，而且讨论起来花费的篇幅比起GC怕是只多不少，所以在这里就不多说，姑且承认异步模型的合理性。在这种情况下，让对象的生存期可以延续到Finalizer返回之后，至少表面上是有一定理由的。当然，这个理由到底站不站得住脚又得另说。我的观点认为是站不住脚的，这样的设计纯粹是为了偷懒。而Finalizer作为GC唯一一个可以用来给自己擦屁股的机制，在Finalizer的设计上偷懒，导致的混乱和麻烦自然就不用多说了。

#### 4. 管理能力和管理效果

这两者不完全是一回事。内存资源大部分时候都是交给应用程序自己管理的，GC可以方便的获得对内存的控制权。但内核对象就不行了，应用程序级别的GC当然没可能去回收内核对象，只能靠Finalizer来补救。即使内核级别的GC，也不可能回收不属于这台计算机的外部资源（比如数据库连接）。

当然，提高管理能力无疑可以改善管理效果。比如说为什么文件（目前）不适合GC——因为文件句柄/文件描述符是内核对象，占用内核态资源。如果有内核GC，就可以在内核对象吃紧的时候启动回收机制回收掉文件句柄，那么这个应用场景就得到改善了。

但归根结底其实还是GC自身的问题。（至少现在的）GC并不知道文件句柄是比内存更珍贵的资源，也没有针对这类资源的特殊策略，更没有提供合适的扩展性（除了那个Finalizer）使得用户可以根据自己的需要来扩展GC的策略。

一类糟糕的抽象的共有特征就是它们在存在大量抽象泄露的情况下，仍然拒绝向用户提供合理的扩展和弥补机制。这个问题不仅限于GC，而是广泛存在。

#### 5. 互操作性

这个东西真是巨大的槽点……说真的，我真是觉得C++最大的一坨翔就是它的互操作性，其他的大部分脑残问题跟这个一比都显得算不了什么了。

都知道C兼容性是C++的历史包袱，class也是槽点众多，问题是，去掉了C ABI compatibility和COM Interop，C++还剩下什么是能跨模块互操作的？

假设一个GCC/Clang的用户，拿到一个第三方库，发现它们在API里用了高大上的std::shared_ptr和std::unordered_map，而且还是用的高大上的ICC编译，然后你手上既没有源代码（of course）也没有ICC（太贵了），他能怎么办？除了骂人之外我想大概也就没有别的办法了。也许抄起调试器来RE一下算是一种办法？

这都还好，最好是祈祷这个高大上的库没有用同样高大上的C++异常，否则的话程序分分钟跑飞给你看……

C++各种堆到天上去了的语言特性，但在最阴暗丑陋的角落里，却是靠着C ABI compatibility和COM Interop维持着和现实世界仅存的联系，不能不说是一种讽刺。

几年前那个说shared_ptr四宗罪的那个，透过现象看本质，还是互操作搞不定。接口一旦染上shared_ptr就完蛋了。结果本来明明是个好东西，用起来却只能小心翼翼，有的时候依然逼得只能自己撸个山寨货出来。

其实只要互操作能搞得定，语言层面上的事情有些时候没那么复杂。Java已经快给搞的不是人用的了，照样还有人搞出Scala之类的语言继续在JVM上跑得欢。CLR更是什么阿猫阿狗语言都能过来凑一脚。C++到了真的写不下去了换一种语言继续写还不行么——现在还真的不行，理由上面已经表达得很明白了。结果就是随着C++的发展，什么模块化，什么解耦，已经越来越变成一种扯淡了。所有的库都变成了一坨坨的头文件，在编译的时候统统#include进来。牵一发而动全身。

现在的C++，语言层面上加点新特性倒还行，想改点什么那是万万做不到的——各种库分分钟罢工给你看。至于说要动那些涉及到运行时的东西，就更是只能呵呵呵了。

#### 6. 安全性

这个问题我觉得你没有get point。语言提供的抽象和运行时实际的环境不是一个概念。

比如我们就说内存。

基于安全性的考虑，* 运行时 * 无论如何不可能允许用户直接管理 * 进程地址空间 *（硬件安全性、虚拟机设计和操作系统安全策略等的合理性不在讨论范围，当作既定现实加以承认）。至于语言抽象层次上的那个“内存”到底是啥根本就不是重点。

假如你还是不能认同，那么不妨换个角度考虑：* 从语言层面上 *，Java/C#有没有可能使用手动内存管理？

这个问题的答案当然是“可以”。其实从语言的角度完全可以这么理解：“它们默认的new操作从一个带有GC的堆里分配内存”。那么我们（就语言层面而言）可不可以扩展一个non-gc new，以及一个与它配套的non-gc delete出来？当然也是完全可以。而且说白了，跟语言的其他部分其实也没有什么冲突。更何况，Java姑且不提，C#又不是没有unsafe和Marshal.AllocHGlobal之类的东西用。

问题在于，从实现层面上，运行时会不会允许你这么干？

这个问题的答案当然是“不可以”。为什么不可以？道理也很简单。万一一个对象还在引用，你就瞎搞把它delete了，怎么办？野指针一出，类型安全什么的统统见鬼去，程序分分钟undefined behavior。

那么假如你是运行时的作者，你又要怎么解决这个问题呢？答案是这问题没可能解决。只要一个对象还在被引用，就不能允许它所占用的内存被其他对象分配，这是类型安全的强制要求。跟用户到底有没有去调用delete半毛钱关系也没有，只跟实际上这个对象还有没有被引用有关——看吧，绕来绕去绕了半天，我们还是绕回到GC上来了。

* 只要运行时对类型安全有强制要求，那么GC就是必须的，和语言层面没有任何关系。 *

#### 7. 人

毫无疑问，人的因素永远是最后的决定因素。不动脑子的人写出来的程序永远只能是垃圾，这和GC或者其他什么东西都无关。

### 2015-04-08 12:01 幻の上帝

1.不讨论deterministic GC，是因为这确实是另一回事，只是称呼可以相近而已。

2.不是零开销是C++的缺陷，和no overhead principle矛盾。根本原因是destruction依赖对象语义，而完整对象在抽象机语义上必须有运行时开销（比如大小不等于0），只能靠优化解决。理论上是容易改变的，只是C++不敢——修改的是最核心的部分。现在C++都不敢在类型系统上直接做手脚，加什么新的玩意儿都不会像加入引用类型那样修改类型系统的基本分类，更别说比这个更加基本的东西了。

3.异步如何设计，这个确实没有硬性规定。但我仍然认为实现上不得不用finalizer变通的设计，不是合理的设计。

4.我提过了是否能GC得看具体资源的性质和使用的需求。如果要让GC在文件句柄/描述符这类资源上也能体现作用，特殊设计看来是不可避免的；但是另一方面，就文件句柄/描述符这样的资源，自身的含义已经相当取决于系统的整体设计了（*NIX就everything is file，但WinNT不是），还没那么容易一概而论。

5.C/C++和其它语言都需要关心二进制互操作性。C勉强能用是因为“事实标准”相对统一——很多时候只管体系结构的调用约定之类就可以解决问题；但这不保证所有情况都能解决问题；这也并不是C提供的能力，而是实现——广义来说也只是运气而已。对于C++，同样这里不保证一致的ABI，尤其恶劣的一点是自身实现之间都无法兼容，几乎不存在C那种“事实标准”，导致实际问题的恶化。虽然二进制兼容被滥用是个更大的问题，C++在设计语言特性时忽略这个需求也无疑是有问题的。更广义地，这里在“易用性”的问题上现实体现的区别无非两方面：是否提供了足够的糖来减轻编码负担，以及是否提供一致的实现让用户容易找到假设。以二进制互操作举例，两者正面和反面的例子可以是（C# v. Java）和（C vs. C++）。

那么相信我，真正意义的“互操作性”在现在几乎所有语言里都是翔。

为什么说是翔？因为这些方面纵然使用上有区别，也都是表面现象，并没有解决根本问题——语义的不完全兼容：用户需要明确的一些关于互操作的假设，语言并没有提供。

C ABI compatibility正是这种错觉之一。事实上，C在语言层次上直接提供了能实现这种ABI compatibility的设计——compatible type。大概因为在类型检查上有缺陷，这种设计被C++抛弃了。nominal typing的需要容易被理解，却没有同时在更底层提供替代抽象（以及明确约束布局之类的功能），那可以说是C++太逊了。但是话说回来，C也就是类型系统上提供了这种机制，怂恿实现“可以那么做”却没有说“一定这么做”，结果就是只剩“事实标准”而已。这种只能依赖“事实标准”的状况，除开相对不容易遇到随便使用现有代码就不兼容的问题（对，仅仅是相对，C也可以有符号下划线前缀兼容之类的ABI问题）外，本质上的不靠谱同样憋屈。所有C的直接和间接的用户不应该沉醉在这种“部分可行”的现实中，这对谁都不是好事。另一方面，因为C++本来就各种不行，所以破罐破摔反倒干脆，根本没有这种幻想了；也正因为这样，我觉得迂回到COM之类实现兼容相当恶心——只是让二进制兼容性从依赖语言实现转到了依赖特定体系结构和系统的组合而已，本质上根本没法保证有自卖自夸的“语言无关”，这远不如JVM/CLR这样有明确规范的中间层。

如果要说根本原因，恐怕仍然是C的“对象”这种抽象能明确的细节太少，和关心二进制互操作的用户普遍期望的东西有很大差距。在可预测的未来，C仍然不太可能会在这里添加内容（不管是定义一个对二进制互操作更友好的替代，还是加上更多的约束）满足仅仅部分用户的需求，所以当大部分用户没有觉悟要求改变时，这里只能忍。

其它语言呢……抛开更上层的应用虚拟机这样的运行时环境不论，二进制互操作几乎都依赖C，或者C的特定实现，所以认命吧。

6.语言抽象保证的形式上可证明的安全性和运行时提供的安全性当然不是一回事。但是，目的是一致的。

基于实现和使用上的成本，前者在许多情况下是值得优先考虑的情况。只有在完全不可信、没法有进一步假设的环境中才不得不使用运行时检查，最极端的部分由硬件提供实现。这往往涉及语言的具体实现，也不能保证各个环境中行为都一致。例如，要提供“访问违例”或者“段错误”的功能，就要有MMU和操作系统的配合；在没有这样机制的实现中就没法指望。

除了行为依赖实现而无法抽象到语言中，这还造成了一些问题：重复检查的开销。

比如说，为什么许多语言的运行时需要检查越界？因为在检查越界之前，无法证明这不会发生而导致预料外的结果——甚至即便用户其实能保证不发生这种状况。这里的解法对于静态类型语言来说非常很简单——C和C++有内建数组类型都堪当大任；反过来，对于动态类型语言，就可能得用一些黑科技来优化了。当然，C和C++“信任用户”而允许UB造成的麻烦导致实际上不能靠这个保证“安全”，那是另一回事。

上面的例子并非说静态类型一定就更加优越或者运行时检查就应该完全被取代，而是指出在实现（而不是保证）安全性时，需要考虑可行的优化方法。一涉及安全需求就把责任推卸到运行时，这是一种不负责任的鸵鸟政策。——要是运行时真出bug了呢？

从这些安全特性上来提供安全保证，这是C/C++这样允许UB的语言中最不靠谱的部分。但是应该认识到，允许UB这是一种在实现可行性和效能上带来极大优势的风险投资——所以说彻底避免也不可能。这样，让实现机制和运行时可选地在必要时提供附加的检查来保障安全性，这是可以接受的。这也应该是C11 Annex K的出发点。

把“可选地”去掉，就成了这些“更安全”的选择了。但是，在没法彻底解决丧失允许UB的优势的问题时这本质上就是一种附加的限制。如果用户真能避免UB而利用这些优势，那么这里附加的安全性就和摆设差不多，却确确实实地让最终用户付出本不必要的代价——在硬件能避免过热爆炸——这种最基本的物理安全性能得到保障的前提下，这种附加的安全性真的总是值得吗？

### 2015-04-08 12:02 幻の上帝

@Tippisum .嘛，关于这里的安全问题，lint什么的我是不怎么关心不过希望sanitizer能成为以后开发测试过程中的标配。

### 2015-04-08 14:16 Tippisum

1. 我觉得我们讨论的前提似乎还有微妙的不一致。

这么说吧，如果区分语言抽象和实现细节是有必要的，那么在我看来，语言抽象的层次其实只关心：到底是手动资源管理还是自动资源管理。

剩下的，不管是基于确定释放策略也好，还是基于延迟释放策略也好，说白了都是实现细节。
极端的例子，RAII算语言层面的东西么？——虽然它确实跟C++联系紧密，但认真来说，它自己本身其实也是实现细节。

* 本质上来讲，RAII其实是个自己homebrew出来的抽象，用来解决C++没有提供的资源管理语义这样一个问题。 *
 
所以说GC到底好不好，（在我看来）这个问题讨论的前提已经是“GC这样一种具体的实现策略到底好不好”。跟语言抽象的层次其实没有多少关系了。

在这个前提下，我想我们的观点总体上是一致的，那就是不问具体情况就一概应用延迟释放策略，至少算是premature optimization。

但GC显然不代表全部的自动资源管理。GC有问题更不能等同于说手动资源管理就最好。毫无疑问，自动资源管理必然是未来的发展方向，问题的关键还是在于具体的实现细节到底要怎么做。（我认为这应当也成为一个讨论前提。如果你觉得手动资源管理是发展方向，那我实在是不知道接下来该讨论什么了。）

当然，虽说是“实现细节”，但这个实现细节毫无疑问是至关重要的。如果不能保证抽象没有泄露，那么提供合适的扩展和弥补的手段（比如说允许用户自行选择释放资源的策略）就是一种义务。而大部分基于自动资源管理的语言显然都没有做到这一点。

2. 还是Finalizer。

“设计上不得不用Finalizer”，其实问题不在这里。

* 资源管理总是有限度的 *。最简单的例子，任何资源管理器（无论它应用什么策略，无论它是用户态还是内核态），都不可能自动管理不属于这台计算机的外部资源。（e.g. 数据库连接）
要完成这个抽象，必然需要一个可扩展的点存在。对于C++，这是destructor。对于Java，则是Finalizer。在这个意义上讲Finalizer的存在本身没有任何问题。你就算不用GC，不用Finalizer，仍然会需要一个机制来告诉资源管理器“应该如何回收这个资源”。说白了，这跟“任何抽象都必然会有某种程度的泄露”是一个道理。用这种泄露的存在来否定抽象本身我看来是不合适的。

问题的关键还是在于，本质上，某些资源是比内存更宝贵的——比如说内核对象，比如说数据库连接，甚至比如说互斥锁。但目前的绝大多数自动资源管理，它不管这些有的没的，不提供任何机制保证这些重要的资源能够得到尽早的释放，也没有在抽象泄露的时候给用户提供扩展和弥补的手段。这从根本上来讲其实还是个“实现细节”的问题，上面已经讨论过了。理论上并没有什么东西禁止资源管理器优先回收这些明显比CPU周期更重要的资源，只不过没有人做而已。Java假装根本没这回事，C#稍微好一点，弄出一个IDisposable，但最后的结果还是把问题丢回给用户。
至于对象死了又活，其实跟上面讨论的话题比起来，是个更次级的问题了。真要说的话，设计Finalizer机制的时候偷懒，搞出一堆乱七八糟的麻烦。但其实也仅此而已了，没有更多值得讨论的地方。

3. 互操作性

是的，大部分语言的互操作能力都是翔。我完全同意你这句话。

为什么？因为语言是平台无关的（至少大部分语言设计上如此），而互操作是平台相关的。
* 互操作总是平台相关的。 *

道理很简单，有平台才有互操作。在一个理想的世界里，整个计算机上只运行着一个由某语言编写的程序。这个程序里也只有一个模块。这样当然没有互操作问题了。
——别笑，实际上绝大多数语言的设计者在设计语言的时候可能都是这么想的。至少，我觉得C++委员会里肯定有不少人是这么想的。何况就像你说的，C++的二进制兼容已经是一坨翔了，破罐子破摔的心理负担几乎可以说是没有。

所以问题的关键还是——语言的抽象和实现的细节之间到底如何找到一个平衡点。有不少语言都过于看重抽象能力（有的时候甚至是盲目的看重抽象的强弱，而忽视抽象是否有良好的定义，是否能够尽可能的避免泄露等，这是另一个问题），而忽视实现的一致性。而C++我感觉就是其中的一个几乎是极端的例子——通过不断的增加各种语言特性，C++的表达能力，绝大多数语言都难以望其项背。但这些强大的表达能力背后却几乎没有多少一致的实现。导致的结果就是只要你还能把所有的库文件都#include进来，那么感觉就会非常的爽。可一旦开始准备要跨模块……呵呵呵……

C语言本身的标准里虽然对于实现也没有加入太多的约束，但可别忘了平台的开发者。把MSDN和POSIX里关于C调用约定的部分都加上，恐怕还不能简单的说C ABI Compatibility只是个“错觉”。——毕竟互操作是依赖于具体平台的。
至于COM Interop，是的这东西真的极其傻逼，全方位的傻逼……但问题在于，如果C ABI Compatibility也是“错觉”的话，那它就真的是唯一一个确实有良好定义，而且确实能用的ABI标准了……

至于Java，其实JVM本身就是一个平台。Java只要搞定和JVM的互操作就行，剩下的是JVM的问题。C#同理。

4. 安全性

当然，安全性可以只是一个feature，也可以是一个requirement。这对应不同的应用场景和不同的需求。对于C++，反正一段C++代码已经在你的计算机上执行了，那你还是认命吧。C++运行时对于一个C++程序到底会不会把自己跑飞或者把整个系统跑飞是一点也不关心的。

这当然也没什么不好，但不同的需求肯定也是存在的。总会有不能允许你随随便便就把整个程序或者甚至整个系统跑飞的场景存在。这种脏活总得有人干，要么运行时来干，要么就得操作系统来干。（其实操作系统也可以看作是运行时的一种，或者说其中一个层次。这又是另一个话题了。）

总之，在这里我想要表达的是，在某些场景下（例如安全性是一个requirement的情况下），运行时必须要承担起管理资源的责任。自动资源管理在这时是必须的——这是设计要求。在这之后才会谈论到语言抽象层次这样的问题。当然，这里的要求仅仅是“自动资源管理”，至于实现是否一定使用GC，这属于实现细节。
前一篇回文里举的例子主要是为了说明，如果设计要求运行时不允许undefined behavior，那么必然导致运行时不可能赋予用户完全的自行管理资源的能力。即使用户使用的语言原本存在对应，在这种情况下也不能使用。（关于这一点可以参考C++/CLI。虽然C++/CLI有些微软特定的东西，但事实上自己尝试一下的话也会得到类似的结果。无论如何，不可能不修改new/delete原有的语义。）

至于“实现”而不是“保证”安全性的场合，如何在安全的同时获得尽可能高的效率，以及这两种场合究竟应该如何划分，这些也都是很大的话题，这里就不多说了。

### 2015-04-08 16:35 幻の上帝

@Tippisum .

1.语言抽象需要关心到底是不是允许用户手动管理资源（至于自动管理机制，只要提供资源抽象就应该提供，因为全手动太容易错）。

这取决于用户需要什么。对于通用目的的语言毫无疑问是肯定的，因为可预见的未来内，完全自动没法保证顶用。

和之前强调的一样，“GC这样一种具体的实现策略到底好不好”作为通用手段，不好。

当然，不必要的手动管理和实现得烂的自动管理一样会造成麻烦，是不恰当的做法，并且可能比GC更糟糕（比如说，实现错的情况下——泄漏了，或者直接挂了）。

RAII只是个可以用于自动资源管理的idiom，是实现的实现。

2.资源管理实际上比这里之前说到的更广。

之前讨论的RAII这样的determinstic释放策略是基于用户可以并且负责确定资源管理的所有权的假设。实际上GC用到的启发式释放策略就无视这种假设。如果约定附加的接口接受运行时确定的所有权，那么实现自动管理原本不在计算机上的资源是可行的。

只不过这种indeterministic很难推广成一般情况，也只有GC这样的特例相对成熟一点，所以普遍情况下不多考虑。

在自动管理中需要finalizer/destructor很容易理解，否则回收的方式只能写死在语言内建规则里，无法扩展。

至少需要finalizer或destructor作为自动资源管理的基本抽象之一，这点我没有否定。
我要说的是，即便回收资源这个目的相同，选取哪种作为基础是方法论上的根本不同，并且我不支持以finalizer作为基础。

或者说，我认为在合理的设计中，destructor这样的deterministic机制总是应该被用户（不止是语言的设计者）考虑到，即便退化成trivial/no-op；而finalizer则可有可无，因为从destructor到finalizer，只是通过限制了资源的种类来换取允许indeterministic以减小释放开销这样的优化。

其实从finalizer加上determinstic的限制得到destructor，或者从destructor为基础加入indeterminstic性质得到finalizer，两者在逻辑上都是能够自洽的。就好像牛顿第一定律在数学形式上是或者不是作为牛顿第二定律的特例，都说得通。

那么区别在哪呢？

finailizer的设计者（以及一些搞并发模型的研究者）可能认为世界的本质就是indeterministic的，所以建立抽象时，首先尽量少预设它们的属性，包括资源什么时候应该被回收。

但我和其他大部分人不同意这种抽象的方式。物理世界是该indeterministic，但宏观上并没有一致地、普遍地体现这一点；更重要地，这里建立的抽象是人为的设计，应该为高效简洁地解决问题服务；否则，这种抽象自身就欠缺意义。这具有更明确的目的和效果。

具体一点，就是大量“比内存更宝贵的”的资源了；更确切地讲，是indeterministic释放会导致问题的资源。

因为这些问题，我认为坚持不提供deterministic作为基础机制的自动资源管理在根本意义上是错误的或者至少是次等的设计；这里的缺陷要靠手动管理其实是一种妥协。

C#这样只是口嫌体正直而已：它没法回避deterministic资源释放这种根本需求，却又不愿意放弃之前依赖的已经被硬编码到语言规则内的GC，所以就搞出了一些不伦不类的接口。这正说明了现实中使用indeterministic自动内存管理作为基本机制是错误的设计。

而Java……脸皮厚还是历史包袱重吧，直到try-with-resource之前的deterministic就只能全靠手动了。

对象死了又活的问题，我只能看出设计者自己不知道在干啥。

3.关于C++的互操作性的现状，实际上有一些比较微妙的地方。

首先不管怎么说，ABI兼容性确实造成了很多没法回避的现实问题。作为尝试，也有proposal开始挽回这个失败。虽然短时间是不用指望了……

另一方面，现实中的ABI兼容问题和厂商的实现质量相关。例如G++使用Itanium ABI，至少确定体系结构、版本和选项内的兼容是没有问题的；而且因为有明确的ABI文档，其它的编译器如Clang也可以做到二进制兼容。

而M$的编译器这样使用没有公开规范的ABI的实现就比较拙计了。clang-cl还得靠逆向……

某种意义上C++倒是提供了一个下限：如果要设计和实现一个新语言，只要不彻底放弃这方面的需求就必须不能做得比C++更烂——C++的现状勉强地被接受并被COM之类的半吊子方案变通，而更差的情况下用户的耐心的不乐观了。

确定的实现下C确实可以具有一些互操作性，毕竟基本依赖的是体系结构的ABI，不是完全看实现厂商的心情，远不像M$ C++ ABI那么含混。不过，不确定的实现下，还是没有保证；或者说，就只是因为C的实现厂商没有像C++那样没节操私藏ABI规范的惯性，基本上也都按体系结构指定的ABI规范来做，能乱搞的地方没C++那么多，所以才相对靠谱一点。

也仅此而已。

最残念的是，这玩意儿即便没有完全清晰的定义，确实也比大部分C++实现干净并且能用……

4.如果说按照设计要求，安全性的外延确实包括了“禁止泄漏或者重复释放”这样对资源的错误操作，那么这里应该没有什么疑义了——保证这样的安全和允许某些undefined behavior确实形成了直接的矛盾；所以C/C++在这个意义上确实就是“不安全”的。

要避免这种问题，无非就是禁止手动管理，或者在每次手动管理之后加上重复的自动检查，这最终会退化到完全依赖自动管理和不使用手动管理的策略上。

在自动管理完善之前，只是自动检查仍然可能通过隐藏在运行时中，而并不一定在具体的语言设计中体现。比如说，C++这样允许UB的语言在需要这种安全性的场合中，通过在不被信任安全的程序的运行时追加检查来确保一旦发现UB就立刻终止，以达到最低限度的安全性。

这明显比提供虚拟机这样完整环境的实现来说限制更大，不过我认为在靠语言直接确保自动管理的机制没法令人满意的将会长期存在的现状来看，还是有推广应用的必要。（虽然可以预料到没有语言规则的限制有不少人就直接扔掉“安全性”了……这也是人的问题。）

### 2015-04-08 18:44 Tippisum

1. 第 1 点基本上同意。这部分应该没有什么太大的分歧了。

2. 看来我们对Finalizer的解释有差别。我并没有刻意强调indeterministic这个特性，因为我认为这是GC策略的问题。Finalizer在这个设定下只是GC的一个扩展机制。换句话说，反对Finalizer，本质上还是反对indeterministic的释放策略。Finalizer本身并没有什么特别值得一提的地方。当然，在indeterministic的问题上，我想我们的看法大体上是一致的。

3. 互操作性牵扯到的方面确实是非常多的，其实不光是C++，大部分语言在设计的时候也都没有怎么考虑过互操作性，最后只能依赖实际上并没有那么靠谱表达能力还相当捉急的C ABI Compatibility，抑或者是怎么看怎么让人觉得傻逼的COM Interop。要么干脆就放弃互操作，直接玩序列化了（*nix底下那发达的管道系统毫无疑问有解决这类问题的因素在里面……）

4. 在可预见的将来，以完全的权限（=把整个程序或者整个系统跑飞的权限）执行的程序仍然是主流。这种情况下，安全性无疑只能主要依靠语言的合理抽象和各种静态检查机制来完成——反正以C++的设计思路，凡是涉及到有运行时开销的特性，都不太会主动往语言里加，更不用说拿来强制了。这里真的只能看人了。

至于你提到的运行时隐藏的检查，我对此持保留态度。因为在大部分情况下，C++是个没办法（强制）保证运行时类型安全的语言。在不改变二进制代码的前提下，很多UB运行时几乎是没有办法发现的——好吧，其实运行时到底有没有能力确定这玩意儿它是不是一个C++程序，这还得要有说。总之，C++运行时对C++程序的强制力几乎可以看作不存在。换句话说，这东西纯粹就是看自觉，不自觉的人总有各种办法可以绕过运行时的限制。问题就在于——如果是一个自觉的人，那么这种设计本身也就没有必要存在了……

所以说安全性这个问题还是有点特殊，基本上它没有什么中间路线好走。要么你就选择相信用户，提供各种语言设计和静态检查的手段来辅助他。要么就干脆点，直接上全套。

### 2015-04-08 16:35 幻の上帝

@Tippisum .

1.二进制意义上另当别论，没有考虑到互操作性不意味着没有互操作性。
极端的例子，暴露AST的Lisp方言在一定意义上就是有互操作性的；当然，这本身和二进制没有关系。

Shell语言算是另一类例子，进一步的缺点是和依赖特定系统的特定任务管理机制提供的抽象。
二进制层次上的互操作性根本来源是体系结构的规范，这对于任何语言都是“外部”的东西，所以在设计时没有特别考虑就不会自动做好。

直接发明通用语言描述二进制接口有很多麻烦。现实比较可行演进做法也许是把现有一些和二进制操作直接相关的底层的DSL（比如寄存器分配语言）加以标准化。不过因为用户群过少，这也比较困难。

只是这里的未来多少被注定了，迟早得解决。

2.广义上的安全的话题说到底也就是可信性的问题问题。所有技术手段到后面都需要人的判断。
只是，当大部分用户自己没有判断的基准时，提供什么样的默认策略就成了之前的问题。
当用户对允许计算机信任程序的判断都值得怀疑时，提供再多保证的安全策略也是没有多少实效。

对于典型的最终用户，权限模型还不见得有提醒用户风险的清晰的UI作用更大；对于专业用户，即便非强制的规范起到的作用也可以更显著。

总的来说，考虑可用性，现实的目标是“让用户允许自己想要的行为”和“禁止不受信任的行为”。
在用户无法保证总是做出正确的精确判断时，这两点有一定冲突的地方。

设计系统时，要考虑这两者的冲突合理地小。通用目的的语言实现也不例外。

运行时隐藏检查的前提是用户明确需要这种行为；这不是反作弊程序，强制覆盖不是这种机制的本来目的。

当然，分辨可执行映像标识特定模式的二进制代码是可以做到的。这样，运行时系统可以提供附加的功能帮助系统管理员对程序运行实施一些强制策略；不过这种机制的实现以及系统管理员的判断是否能保证安全，那又另当别论。而且这个是另外的机制了。

不自觉的问题体现在——比如说——开发者可能根本就没想到使用这种机制帮助提升安全性。

中间路线也是有的，但不是作用在相同的地方——分层设计。现在CPU和操作系统使用不同特权级执行不同风险的程序就是典型例子。只不过典型实现的二进制层次和接口抽象上都只有内核态和用户态这一个边界，没法被大多数应用直接使用，还是太粗糙了。这时候运行时进一步在受控的执行环境虚拟出了这种机制，这可以说是一种变通。

### 2015-04-09 09:58 Tippisum

1. 确实，没有考虑互操作并不必然导致没有互操作。因为巧合或者运气或者事实标准的存在，即使设计时没有考虑互操作，现实中多少还是可以有一定的互操作性。只是这种互操作性终究不可能与具有确定定义和规范的接口相比。而对于后者的需求，随着现在计算机系统的愈发庞大，也终究会愈加的迫切。如JVM/CLR等路线的出现，也无疑带有解决此类问题的相关考虑在里面。
除此之外，使用解释型的语言，或者使用序列化手段，在某些场合也可以作为simple and stupid的快速解决方案。例如你提到的Shell脚本，抑或者是FastCGI之类的接口。二进制序列化目前看来仍然是人与人之间最后的信任，只要对性能没有极端的要求，实现也不要自作聪明的做一些奇怪的事情（艹换行符/字符转义啥的），在当下大多数场合里还是能对付过去的。

2. 安全问题的复杂性在于安全性本质上来讲是一种利益。如果我们把开发者和最终用户分开来看，他们的利益显然并不总是一致的，不能一概而论。如果要从“可信性”的角度来说，那就是对于这两者而言，他们的“信任”并不是一个概念。

当然，这两者之间也有（很大的）重叠，开发者和最终用户之间的划分往往也并不绝对。但这会有助于理解一部分安全性设计。

在这个基础上，运行时提供的安全性主要覆盖的是最终用户的需求——对于最终用户而言，他们显然无法受益于（开发者机器上的）语言所提供的任何静态的分析能力，甚至他们不能肯定开发者是否会为了达成某种目的而有意绕过这些检查。我所说的“不自觉”，便带有一些这方面的含义。像这种“不自觉”的开发者究竟有多少，想必你也是很清楚的。因此，运行时提供强的设计安全保证，不仅仅基于技术原因，也有policy的考虑。这也正是我之前回文里提到“这是先于一切语言层面（或者说的更明白一点，一切技术分析）之前的设计要求（policy）”的原因。

事实上我们完全可以把操作系统也看成是运行时的一种，一部分，或者说一个层次。这一点我也在之前提过了。所以说所有基于硬件的安全性本质上也可以归入运行时安全性。至于由软件实现抑或由硬件实现，其实是没有差别的。如果我们把操作系统看作运行时，那么它毫无疑问确实是使用自动资源管理的，只不过目前自动化程度还很低，只能在用户进程退出的时候才主动执行清理操作罢了。而且毫无疑问，它确实是不允许用户进程自己管理内存（RAM）的，只是策略并不是GC，而是硬件分页。所有的这些和我在之前的回文里提到的观点——语言抽象层次上的那个“内存”其实并不是一个真正有意义的概念（它其实说白了也只是一个资源抽象），以及任何时候只要存在强制的安全策略，那么就必然有相应的资源管理与之对应——都是符合的。

反过来说，对于开发者而言，他掌握更多的资源，在这种情况下，如何充分利用这些资源当然就成为一个值得考虑的问题。既然有静态分析，那么把问题拖到运行时去检查，浪费额外的时间和计算能力，显然不是一个聪明的做法。关于这一点，我毫无疑问是同意你的观点的。

当然，整个计算机体系的安全性设计本身是个非常大也非常有争议性的话题。但目前的主流操作系统在这个问题上的设计策略基本是一致的，即便这些设计已经比绝大多数语言还要落后于时代，在有足够强大的力量推动之前，只能将此作为既定现实来加以考虑。也就是说，目前从实用的角度上，不可能存在从操作系统的基本安全性强制的基础上进一步“以安全换效率”的设计。基于这一点，不难理解C++这个语言在设计上就没有打算让运行时提供任何操作系统本身没有提供的安全性机制，其目的自然也是在既定的操作系统框架内，取得最大可能的灵活性和工作效率，允许各类UB等设计也与此相一致。相反，Java/C#等语言则是在设计上对“安全性”更加强调，因此才会有JVM的沙盒机制和CLR的各种附加权限控制。这些控制，根据我们上面已经进行的讨论，很容易得到——它们必须要有一个完整的虚拟机和与之相应的资源管理机制，才有可能实现。C#为什么会有unsafe，其实也是如同你所说的，口嫌体正直。M$当然知道强制运行时来检查安全性，在某些场合效率不够用。怎么办？unsafe，意思其实就是“你既然要作死，那我就不管了，跑飞了也不关我的事”。有了unsafe，手动内存管理，去除边界检查，该有的都有了。要不要作死，就看开发者自己了。当然，最终用户也许不会答应，毕竟不见得所有的最终用户都愿意让一个不能保证不会作死的程序在自己的计算机上跑——所以又回到了我在这里最开头提到的问题，开发者和最终用户，他们眼里的“安全性”，不见得是同一种东西。

PS，其实这个问题还有一种可能的解决办法。如果编译器通过静态分析，可以证明某个数组访问一定不会越界，那么它可以为此生成一个证明。这些“优化证明”可以作为程序本身的一个附属文件。如果运行时在执行程序的时候读取了这个证明，就可以取消对相应安全策略的强制应用。不过感觉这东西也属于可预见的将来没戏的东西……

### 2015-04-09 12:11 幻の上帝

@Tippisum .

我不同意先验地区分开发者和最终用户。一个合格开发者基本上总是最终用户（负责做的东西自己总要过一遍），而一个清楚需求的最终用户和开发者在考虑接口设计时的作用相同。

开发者和非开发者眼里的东西可能相同，也可能不尽相同。一些开发者结果上会和最终用户的不同，只是说明作为用户的开发者期望和其他用户不同的产品，附加了不同的需求而已。

无论如何，这种差异已经脱离具体的单一系统了。

就不自觉这点来说，大部分用户，不论是不是开发者，都没有什么区别。

然而决定使用什么样的信任策略，根本上是用户自己的事，因此当自己不是目标用户时，我不喜欢使用强制策略。

操作系统毫无疑问是能有效实现这种强制安全策略的一个层次，但不是所有操作系统都有必要做到一样的程度。

多任务的操作系统倾向于严格的安全策略不仅仅是因为用户（系统管理员）在这里的认识局限，关键的问题是本来就没法信任所有程序，而一个不可靠的程序让系统中的所有其它任务失败是不能接受的，因此运行时采取保守的策略保证整体可预期的行为。

说到底还是看需求。

硬件实现和软件实现的最大区别在于，前者的可编程性差，比较容易做到排除物理手段以外的途径无法绕过，甚至物理加固。即便只是防止泄密，直接存储的密钥也相对不靠谱。只不过一般人都没条件随意破解这些技术所以借助用户自然的物理隔离就可以基本不考虑罢了。

这不表示没有改进余地，只是无论改动芯片设计还是操作系统，实用上的成本都太大了。即便如此，语言的实现也就是顺其自然而已；要是真的改到支持了附加的安全性（只是支持，不一定用起来方便），也不必要修改语言，对于C++这种来说只要去除一些UB限定一个子集再补充一些API即可——前提是真能做得出来。

至于Java/C#这样，与其认为强调安全，不如说是对专业人员中充斥安全设计的无知而做的妥协。对于C/C++这样允许UB的语言来说，了解UB的风险并尽量避免是合格的实现者的义务这点就是常识。而对于Java/C#的用户，可能大部分根本就对语言的语义可以不直接保证可预期行为完全不知情。

这样做的结果是开发者怎么乱搞风险都会低一些，但显然不可能全部隔离（依赖UB就只是一小撮不靠谱的bug），却明显限制了原本就重视安全的开发者的自由。同时，代价还有非开发者也需要承担的运行时额外开销，降低硬件的利用效率，这对他们来说不公平，我不认为放任专业开发人员使用这种方式生产软件是合理的。

而仅仅是防止手贱失误，非强制安全策略大部分情况下都已经足够（质量保证流程上当然要强制，但这和不修改程序的最终用户的运行无关），不需要那么麻烦。
如我一贯强调的一样，安全根本上是人为的问题，需要有不同的策略和层次。unsafe这个说法本来只有相对意义，作为语言关键字有一定的误导性，暗示新手好像避免unsafe了就safe一样——其实这里的safe只是很小的一部分，却往往不被注意到。

考虑到语言和实现为此更加复杂，我不认为这种二分法是正确的设计（或者说，这比直接说UB就什么也不管了也没强到哪里去）。合理的设计应该提供更大范围的明确选择：启用哪些检查，指定哪些范围的行为是必须能被预期的——即便这样会使语言和实现更加复杂，但是值。

要求形式验证的profile也是一种可行的安全策略。不过，这需要附加的运行时支持，现有语言实现都没有做到——传统上形式验证使用的语言基本是DSL，用起来都是间接的代码生成，和日常编码使用的语言在实现和用户群体上交集都太少了。不过，这应该是可行的，而且是一个有潜力的方向。

### 2015-04-09 12:24 幻の上帝

多任务系统上的问题，之前应该也有提过：应用程序GC的设计者和用户为了自己期望的莫须有的简单，往往使用了一种对最终用户不负责任的态度——对系统中的其它任务茫然无视——这点让我非常不爽。具体点说，就是一个劲地吃内存吃到物理内存吃完还不放结果引起换页，拖慢整个系统的性能。

上面说过，在多任务系统保持安全性的主要原因是不能容忍运行了任意一个有实现缺陷的程序就引起其它任务的失败。这里对资源不合理的占用，影响整个系统，严重时也可以导致其它任务因为分配不到资源而失败，在一定程度上也是“不安全”甚至不可容忍的。如果仅仅是占用资源大也就罢了，关键是以此为代价，最终用户通常没有任何好处——即便GC本来可以作为对缓存友好、避免集中释放开销而提升性能的优化，但糟到引起换页的情况下，基本体现不出来，性能很可能明显更低。

因为这些原因以及造成的经验教训，我在不保证内存总是闲着没地方用的时候就避免运行Java写的一些大型程序（可能有实用价值的主要也就是某些IDE了）。

### 2015-04-09 13:37 Tippisum

1. 关于开发者和最终用户，我个人也不倾向于认真区分。主要还是之前你谈到了“最终用户”的概念，故而加以一提。而且我也说得很明白了，这两者之间本来也就有很大的重叠，区别也远不是绝对的。

这里我的核心观点是，运行时，无论是“操作系统”这个广义的运行时，还是如JVM/CLR这样的运行时，* 可以 * 向用户提供一个明确而可靠的约束。当然，这不是运行时的必要条件。例如C++运行时就不提供任何可靠的约束。但是，建立 *可靠约束 * 的 * 必要条件 * 之一就是对资源管理的控制。换言之，任何一个实现可靠约束的运行时，都必然要对资源进行额外的抽象。以内存为例，就是 RAM -(操作系统)-> 地址空间 -(JVM/CLR)-> GC Heap 这样的一个层次结构。关于这个问题，在我们之前的讨论里面应该已经很明白了。

至于由运行时提供可靠的约束 * 是否必要 *，这个问题实际上超出了技术讨论的范畴，是个policy的问题，这一点我之前也提过了。就现状来看，显然这样的需求是有的。至于是否合理，是否有更好的替代办法，可以考虑换一个话题继续展开。（稍微多废话两句，JVM/CLR的安全检查并不仅仅是防止手贱，它们确实是实现了比操作系统提供的默认安全机制更加有效的控制。在如GAE等服务器应用中，这些安全机制的效果是可以得到体现的。而根据前述的理由，实现这种安全机制的前提就是强制的内存管理策略。不难想象GAE无论如何是不会允许托管C++代码的。当然，对于“普通用户”来讲，这些东西都等于没有。）

2. 说白了，这里的讨论涉及到两个层面，手动资源管理和自动资源管理相比的层面和自动资源管理内部比较的层面。

首先，基于前述讨论，技术以外的原因导致不能允许手动资源管理，或对此有所限制的场景是存在的，这一点应该没有什么疑问。

如果没有技术以外的原因禁止手动管理，那么语言或者运行时单纯为了所谓的“方便”而主动阉割手动管理，这毫无疑问属于因噎废食的可笑行为，是不负责任、让人不能接受的。
至于自动资源管理内部比较，这个问题也已经讨论的很清楚了。不针对具体的应用场景，盲目应用indeterministic，甚至完全忽视用户可能的管理需求，这样的行为，即使在自动资源管理有必要的场合，依然是不负责任、让人不能接受的。

在这个问题上，如Python等语言，采用的是deterministic的管理机制，这方面的矛盾相对不那么严重。C#默认采用GC，但仍然提供了IDisposable和using语法糖作为妥协，也还算是能用。Java大概就属于典型的反面教材了。

### 2015-04-10 14:39 幻の上帝

@Tippisum .

1.提最终用户是因为需要提到开发者和用户。最终用户也只是特定身份，强调“不作为开发者”时的用户。至于一个用户什么时候是开发者什么时候就一定不是，这点没法完全预料。
以提供约束论，这里开发者和非开发者确实不完全一样。相同点在于，约束都是基于接口（包括UI）的限制。不同点在于，开发者可利用的接口比非开发者更广，会有不同的机制来体现。
非开发者也可能接触到资源抽象，例如设备管理器、资源监视器里面的那些东西。但比起开发者来说，这样的约束容易建立得多，因为给非开发者用的UI即便能修改系统状态，也缺乏可编程性，没法任意组合，直接少写些东西给出确定的有限接口就能应付破坏系统正常运行需要的前置条件和不变量的操作。

开发者接口可能根本就没法这么做（比如ABI），或者照这种办法最后会导致几乎没法使用资源（比如API）……所以退而求其次，让运行时环境给出系统性的限制来约束，而ABI这种没法方便建立约束的东西对大多数开发者都干脆基本禁止了，或者至少强制检查。

不过，对于“信任”这件事本身，这还是实现的程度上的差异。

既然没人能确实地阻止非开发者变成不理智的开发者，自然没法寄望于防君子不防小人的有限接口（比如UI）提供多少完整的安全性。所以更底层的系统约束是必要的。另一方面，如果真的要当小人，再怎么完善的安全措施也没法发挥到底，毕竟肯定有用户能在物理上控制机器。许多情况下他们确实有合理的理由绕过这些安全机制。

所以最后还是得看不同的用户和不同的应用场景。于是观点其实是一样的。
关于具体语言实现的安全问题倒是需要再讨论一下。

JVM/CLR提供的安全机制粒度上确实比硬件和操作系统内核典型提供的机制更灵活且容易被应用使用，但实用上还是远远不够的。

应用需要的专业领域的安全方案通常都有自己的协议，规范也可能是公开的，比如网络通信常用SSL/TLS，而且机制和实现用的语言实现相对是独立的。

在这些需求面前，应用虚拟机提供的机制还是太过于底层了，而且并非不可替代（注意到现代的操作系统提供的机制能基本保证不一出问题就炸，修补以后拒绝服务之类的攻击基本也只能利用应用设计和实现的漏洞）。以至于比起native实现没有什么优势，反而性能等方面还可能吃亏。（当然，错误实现时可能会造成更严重的后果，这另当别论。）

至于托管native代码，并没有不可实现的问题。通过部署虚拟机实现“云服务器”现在是很流行的技术了，当年也足够成熟（虚拟化整机的历史有几十年了）。

这里，应用虚拟机被系统层次的虚拟机以及上面跑的操作系统代替，安全性则通过宿主机的操作系统的进程隔离等更基本的机制来保证。

不可否认，native代码需要的环境比较复杂，维护起来的技术门槛也比较高。这样，资源成本就没法比了。对于GAE这样的服务来说，大概这本来就不是他们想要的东西。不过，其它“云服务”就不一定。

所以两者之间也并不是取代和被取代的关系，同样得看不同需求和场景而定。

而考虑到应用虚拟机的实现其实依赖于更底层的操作系统和硬件的安全机制，JVM/CLR这种高级的上层机制也更像是可选的而不是必须的。

2.同意在通用目的的语言中刻意禁止手动管理是可笑的非技术原因（无法信任专业开发人员）。
（当然也有不管资源抽象的“通用”语言如XML……这类不算。）

对具体语言资源管理的概括也和我的看法一致。

要是DSL就睁一只眼闭一只眼罢了……搞不好目标用户就是原本意义上的非开发者，本来就没打算有这个概念。不过Python、C#、Java这类怎么说也不是当成玩具的东西，搞得这么麻烦/无能就有点不地道了。

### 2015-04-11 17:58 Tippisum

既然打算在安全性这个问题上再稍微展开一些，那么我也再多说几句。

首先还是最终用户和开发者的区分问题。在我的观点里，这里不是用“是否作为开发者”（或者说，是否进行实际的程序编写）来区分。

在安全性的模型里，一个典型的应用场景会有四个主要的角色存在：

1) 计算需求的提出者和执行者

2) 计算数据的拥有者和提供者

3) 计算资源的拥有者和提供者

4) 解决方案的设计者和提供者

另外还有一个额外的角色，那就是体系外的攻击者

其中 1) 是典型的最终用户，而 4) 是典型的开发者。

当然，在实际问题中，这四个主要角色可能部分或全部存在着重叠。因此一个人当然可以既是“最终用户”（提出需求）同时也是“开发者”（解决问题）。这部分就先不展开了。

在理想的模型中，这四个角色是互相重叠的，或者至少，这四者应当有共同的利益。因此理想的安全模型里只有一个主要的目标，那就是“体系外的攻击者”。

但现实却往往不是这样。四者互相之间心怀鬼胎甚至明争暗斗的情况几乎是常态。DRM就是个最大的典型。除此之外还有Trusted Computing和SaaS之流……明眼人我想不太可能会觉得它们的目的是如同它们对外声称的那样吧。

说白了，这些问题的根源，与技术无关，纯粹是来自人类的愚蠢和贪婪。只不过有的时候，这些非技术问题却需要靠技术来埋单，或者说需要拿技术当枪使。所以才会有各种不可理喻的设计产生。归根究底，就如我们之前已经得出的共识那般，这问题实际上根本就不是技术问题，只能当作既定的现实来接受。当然，合理的设计可以在一定程度上缓和或对付这些矛盾，但绝不可能从根本上解决。

回过头再来说说技术上的问题。

native代码托管当然有，而且现在已经被广泛使用。我自己也使用VPS来托管网站。不过，native代码托管和我之前所论述的观点并没有矛盾——它依然是通过为资源增加额外的抽象层的办法来解决安全性问题。特别的，在这里，“安全性”对应的是前述角色 3) 计算资源的拥有者和提供者 的安全利益。显然，他不仅有对付“外部”的对手（网络黑客）的需求（这是个正常的需求），也有需要对付“内鬼”，也就是那些所谓的“用户”（“最终用户”也好，“开发者”也罢）滥用自己所提供的计算资源的需求。（这个需求确实很可笑，但它也确实是客观存在的）

而不同解决方案的不同点本质上只是在于具体使用什么方式来实现这个额外的抽象层罢了（平台虚拟化 v.s. 应用虚拟机），套用前面的说法，那就是这其实还是实现细节。

当然，落实到实现上，虚拟化方案现在有硬件提供支持，某些场合更加高效。但相应的，也有常常需要虚拟一个完整平台的代价。在实际的应用场景中，究竟哪个更加合适，还需要具体问题具体分析，不能一概而论。

“两者之间也并不是取代和被取代的关系，同样得看不同需求和场景而定。”，这一点我完全赞成。

“不过Python、C#、Java这类怎么说也不是当成玩具的东西，搞得这么麻烦/无能就有点不地道了。”

归根结底，其实还是人自己给自己下的套。

最后总结一下观点：

在理想的世界里，用户应当受到完全的信任。在计算机的智能没有全面达到或超越人类之前，应当赋予用户根据自己的判断来进行资源管理的权利，语言和运行时在这里应当起到的是一个合理且有效的辅助作用，而非越俎代庖，“代替”用户来下决定。

然而现实中，由于各种非技术原因，导致不能完全信任用户，甚至从设计上就将用户当作潜在的“敌人”，这种可笑的需求仍然客观存在。而为了满足这种需求，系统必然要在某个层次上面对资源进行额外的包装，从而剥夺用户对资源的完全控制能力。这个矛盾的根源是非技术的，因此不可能单纯通过技术的手段得到完全合理的解决，只能以不同的方式、在不同的层次上达成某些妥协（e.g. 硬件物理隔离 / 硬件虚拟化 / 硬件权限控制 / 系统权限控制 / 应用虚拟机 / etc.）。

回归技术层面，GC本身只是实现自动资源管理的其中一种手段。它既不能代表所有的自动资源管理，当然也不一定（甚至不太可能）是其中最优的手段。在需要提供（或强制）某种自动资源管理的时候，应当针对具体的应用场景进行合理的分析，在必要时向用户提供合理的选择空间和扩展机制。不问前提的滥用GC，甚至强制GC作为唯一的资源管理手段，是一种不理智、甚至可以说不负责任的设计思路。

### 2015-04-11 19:43 幻の上帝

@Tippisum .

区分开发者和非开发者正是为了评估需求外延的必要。

提用户的时候是指抽象的个体，明确角色是为了讨论项目意义上的涉众的必要；然而这是可能会变动的：一个任意的现实的系统中，用户并不总是保证具有单一角色：一个用户可能一会儿是开发者，另一会儿不是。同时，负责整个体系的设计者占绝对少数。一般系统之间各个部分的实现都有不同的涉众来作为不同的角色。一个系统中的某一部分的开发者，到了另一部分就可能只是最终用户。

从项目管理和维护的角度来看，一般意义下理想的模型至少有两个（极端）：所有角色完全重叠使需求理解的成本和交互的复杂性最小化；所有角色完全分离使涉众切换角色的开销最小化。
安全模型从技术可行性和可靠性出发，一般不考虑人的系统内部一致目的涉众的实施效率问题，所以只取前者。不过，既然涉及人的问题，这个模型不总是一定合适。
现实当然不理想：几乎任何一个有超过一个人参与的项目，既不能一体同心，又不能完全稳定地保持分工明确。

由于目的并非一致，有些之间还互相矛盾（如可靠性和实施成本），这里才需要各种妥协。这种大规模情况更是如此。目的上不一致其实也无可厚非（可能本来系统外就是这样的状态，也可能是沟通问题），虽然某种意义上也算是愚蠢，这是要正视的——换句话说，求同存异以让问题能够被多数人解决，这就是基本的一个需求（也因为如此，一个稍微有点规模的解决方案，在项目意义外划分角色仍然是必须的）。

剩下的观点看来基本一致。总结陈词已经做了，我就举例提一下更一般的问题和对现状的看法吧。

使用上面说的哪种技术来实现服务，这由需求决定。而需求为什么会变成这样，与其说是技术问题，倒不如说是具体涉众对问题的认识和立场问题。

提出切合实际的真正需求，其中的一部分总是最终用户的责任。作为对实现稍微有点理解的人员，我在这里就占便宜了。

比如说，我想要VPS这样的服务，首先我明白我自己不会为了这个需求自己就变成VPS服务商（要是有这个条件，对于这个需求就有更好的选择）：我至少现在没有足够的技能、精力和兴趣彻底去搞定所有麻烦事，所以我会闪开让专业的去做，自己首先只是去当消费者，而不是这项服务的生产者。

如果我没有这些可行性的自知之明而贸然行事，效果大概不会理想，我的利益会有本不必要的损失。

这里有个更根本的原因：我虽然不了解实现细节，但我知道评估可行性之前应该有的期望，因此能让需求一开始就相对合理，符合现实。

如果换成一个土豪外行，为了这种需求烧冤枉钱也并不是没有可能；考虑生产出的附加值，结果上他不一定就会亏，但仅仅对于原本的具体需求来说，肯定是不划算的，风险通常也更大。
能怪这样的人傻么？也未必。说不定外行仔细考虑起来花费的时间成本比这些冤枉钱更贵。
需求方自身对解决方案的实现的想象能力，某种意义上也是隐性的需求自身的一部分。

再把话题换成一个更和原来的主题相关的例子说明我的立场和其他某些人可能有的差异。
例如，一个自用的Windows桌面应用，可能用C++或C#实现，技术难度看起来差不多，选择哪个？

对于我来说：

1.如果是作为开发者，去读C#实现的源码乃至修改以适应自己的附加需求，虽然通常也没什么大问题，但也可能容易不爽。

主要原因就是上面说过的，默认的GC语言会让资源管理方面的逻辑变模糊，规模一大就比较疼；还容易罗嗦。

虽然不得不承认现实C++不合格用户更多导致有更多看起来功能实现得好好的实际却是烂代码的风险，但还是容易找到好的、干净的、容易实现需求、节约我成本的代码。

作为有能力提供全部实现的开发者，我不喜欢这里C#表现的中规中矩——因为在我会去考虑实现的规模内，我十分不缺鉴别并扔掉烂实现然后转而寻找/自己实现替代方案的能力。

所以即使C++槽点更多，我在这上面也更不待见C#。

2.然而作为一般用户乃至最终用户，要开发Windows桌面应用，强调解决问题本身而不是尽量挖掘实现和方案的可复用的价值，我会在不十分严肃的场合优先选择C#而不是C++；也包括类似场景有人咨询如何选择方案时。

中规中矩意味着“智商兼容性”，或者一定程度上来说的低风险。尽管远离最优解，使最终用户往往付出了更大的代价，却有更大机会避免什么问题也解决不了的最差情况。虽然消极，但是有用。

当然，卡翔到某些Java程序的方案，不管作为什么层次什么角色的用户，都直接pass：不满足性能需求。

因为是对我来说，这里满足一个假设前提：知道开发人员在怎样的条件下能怎么完全发挥出本来的优势，使其中哪一个更合适。

然而因为各种因素，公认C++比C#在编码上麻烦，而且工具上有欠缺导致开发效率和质量问题，所以许多情况下还不如C#实现。

结果时间久了就变成了在没有预设前提下“C#更适合写Windows桌面应用”这种都市传说。
实际上呢？恐怕存在不会具体分析问题只会盲目重复这种不靠谱结论的这个问题本身，造成的麻烦可能才更严重。

其实专业到一定程度的开发者都应该明白哪个选择有哪些局限性，以及具有在熟悉项目背景以后做出合理决策的能力。

然而“业内人士”有太多云里雾里乱传谣的了。

我在此非常不满的，归根结底就是这一点：大量“专业”人员明明自己了解得不够明白还说大话，有意无意蒙骗外行的用户。

再追加一些的话，还有很多小白半桶水咣当响，不去自己分析问题，反而给会独立思考解决问题的人灌迷魂汤，绑架用户需求，似乎以为只要实现用户字面上提出的东西，做出妥协也是光荣了。

至于一直作为外行用户的，了解这个也不算是义务，所以我倒是没什么好说的。

### 2015-04-11 21:59 Tippisum

1. 涉及到人的问题总是复杂的。

然而归根结底，技术还是要为人服务。因此妥协总是存在的而且也是有必要的。

2. 同意“提出切合实际的真正需求，其中的一部分总是最终用户的责任。”

主楼的问题基本上没有什么分歧了……稍微跑点题。

3. 看来你的反感更多的是来自于不理智的用户和装糊涂的宣传。

这一点上我确实能够理解你的不爽。问题在于能够看透这些东西所需要的智商明显超过了平均值一个标准差以上……换句话说就是随着认识水平的深入，智商能够跟得上的人是指数衰减的，大部分人总会在某个节点上被绕晕或者被忽悠，这个事情你我都无能为力。何况让大部分人都糊涂，显然是符合某些人的利益的——这又是另一个非技术话题了。

4. 关于自己撸实现。

复用性经常是个伪命题，至少对我而言是如此。当然这跟我本人并非计算机专业人员也许有一定关系，对我而言，“不十分严肃的场合”显然更多一些（话虽如此，我想我和你在对于更合理、更有效的实现方面的追求是一致的）。

另外，我比较喜欢使用多种语言混合编码——这大概也是我难忍对于C++互操作性的吐槽的原因之一。很多时候，C++的实现确实更加清晰效率也更高，但那翔一样的互操作却使得这样的实现无法和其他的部分有效对接，导致我经常面临将整个项目C++化和使用其他实现替代这样的二选一。在大部分的“不十分严肃的场合”下，我只能选择次优的替代方案。

PS，其实以我的经验，对于个人会需求一个“Windows桌面应用”的场合并不多。如果只是需要一些简单的GUI交互，撸HTML和Flash是非常方便的。后端可以直接甩个CGI之类的东西，或者如果嫌CGI太阳春也可以自己写点简单的ASP.NET做转发。

### 2015-04-12 06:28 幻の上帝

@Tippisum .

1.不管我怎么想，技术是要为人服务，然而我希望找出技术能尽量自然选择淘汰掉不适宜人群，总是人工选择太费劲……

3.不理智只是一个方面，矛头对准的是没有自知之明，被人指出（对事不对人，即便别有用心也一样）还厚脸皮死不认账的——毕竟我得承认我自己也没法在任何时候都避免不理智。

4.复用问题可以概括为两个方面，发明和制造轮子。

在已知的范围内发明轮子问题我坚持两个原则：(1)只有没找到能满足我当下需求的轮子时才发明；(2)我发明的轮子不能比已经找到的类似物更烂。

换句话说，一些轮子实现比我发明的更好，但我的轮子的存在说明这些实现不够满足我的需求；能满足我需求东西的我还发明了轮子，是因为之前我不了解，等我发明出来又发现（在实现质量等角度上）还不如用我自己的。

这样至少能保证我发明出来的东西不至于全是废物。

至于是制造自己的轮子还是使用别人的轮子，更多是看心情……好吧，是该看成本和收益是否相符。

具体地，比如说语言的标准库我基本都会直接拿过来用，只有少数特定清楚实现前提下的例外（像C/C++标准库转换文本编码就是残的，码表格式和位置还很不方便随便指定，现有实现又很多和特定系统库耦合……既然有特定需求就自己实现了）。

而多种语言混合使用，这和需求更加相关——规模稍微一大的项目就几乎没法避免。比如说C++和其它一些语言的项目可能用到C代码；C/C++项目可能同时用到汇编；makefile/shell之类的用于构建脚本等等（其实IDE一般也自己用到DSL管理项目信息/自动化构建，虽然一般人没事不怎么注意但我会去看），这也是容易理解的通用实践。

虽然基于可控实现的理由也许以后有机会还是都使用自己发明的语言……
PS.拿Windows桌面应用举例是因为我以及周围人曾经有过这类实际需求，并且容易体现出不同选择的差异。如果是Web，随大流就容易多了，基础技术之间没那么分明（虽然兼容性容易疼）。

### 2015-04-12 10:22 Tippisum

1. 虽然我也有和你类似的想法，但总觉得这不太现实。从以往的例子来看，任何技术都有被滥用的倾向，或者是潜在的有被滥用的倾向……

3. 在网络上承认错误是困难的，有的时候和本人是否有自知之明没有完全的联系。当然，死不要脸的人在现实世界里多半也不是什么头脑清醒的家伙。

4. 关于造轮子。总体上来讲，我并不喜欢自己造轮子。对我而言，取舍的标准更加现实：a) 现有的轮子是否足以满足我的需求。 2) 现有的轮子，其接口设计是不是合理。

接口设计不合理的轮子，不仅有自身实现可能一团糟的风险，更有将这种一团糟的设计传播到项目的其他部分的风险。即使我可以选择性的无视前者，后者在很多时候是令人难以接受的。
换句话说，我可以不管你的轮子实现到底有多烂（前提是它得能用、正确、并且性能过得去），但如果接口设计不合理，让我没法不蛋疼的用，我在很多时候是不能忍的。

PS1, 关于自动化构建的DSL，我用过MSBuild和(N)Ant。比起makefile确实是巨大的进步。不过面对变态的需求（e.g. 自动化构建一个简单的操作系统内核）往往还是力不从心。后来我自己写了一个简单的小工具，可以在项目构建时自动执行任意C#代码，感觉绝大多数需求都基本上可以满足了。（所以说对某些DSL而言，扩展能力是个很重要的考察项目。有扩展能力的话用户就可以自己发明一些轮子来满足自己的特殊需求，而不是干瞪眼或者滥用现有的技术。）

PS2, 因为我看到你似乎在造图形库的轮子，虽然我想你在这方面的理解应该要比我深入一些，但姑且还是提一下：对 GUI 的需求和对绘图的需求并不完全是一回事。

大部分情况下，一个“Windows桌面应用”需要的只是前者。只有少数应用会真正需要复杂的绘图逻辑。因此我个人对图形库的设计想法是，对于前者（需要标准化的呈现和交互语义），使用某种DSL来作为描述语言，并辅以合理的扩展和后端交互机制，尽可能的分离UI和实现逻辑。对于后者，则提供尽可能有效的图形抽象，给予使用者最大的灵活性。

其实HTML本应该是很适合拿来干这个活的，可惜被W3C给玩残了。（不知道为什么，W3C似乎总觉得HTML的目标应用场景是用来呈现出版物。问题是连个分页控制都搞不好的东西拿来呈现出版物这不是搞笑么。）JavaScript作为前端扩展语言基本上也没法满足复杂的交互需求。有Flash扩展的话，绝大多数场合我感觉是够用了（而且同时能够满足一部分不是很复杂或者性能敏感的绘图需求），主要麻烦在于Flash的实现基本是黑盒而且跨平台也没那么靠谱。至于WPF，设计想法我觉得是很好的，但实际的实现还是会让人不爽。（M$的通病之一）
对于满足真正的绘图需求，我认为你的主要竞争目标可能不一定是去选Qt，找OpenGL说不定更合适一点。当然OpenGL有硬件支持，想替代估计是没戏，但把OpenGL那翔一样的API封装的能看一点我倒真觉得是大功德一件……

### 2015-04-12 20:26 幻の上帝

@Tippisum .

1.完全实现不现实，但也不是什么都不能做。
……至少遇到逗比教育一下还是可以的。

3.承认某些错误或许一概是困难的，特别是可能有随大流法不责众的心态。比如说，就是有很多人不接受C没入门。

至于是否在网络上，我觉得只要了解是对事不对人就自然没必要区分。虽然不排除有些人特别喜欢伪造人格（？）体现存在感（反差萌？），那我是没辙。

4.第一点和我是一样的。第二点的差别是“接口”和“实现”。那应该是需求不同。

考虑大部分的问题背景，现阶段我遇到的许多东西需要从底到顶较彻底地控制。

这样问题下的“接口”几乎总是相对意义下有效，没个准：某些“实现”，可能深入分析后第二天就变成另一层次上的“接口”，需要同时评估乃至提供替代实现。

PS1.MSBuild和*ant这类XML类似物读起来还大概明白，写起来那疼得……而且参考资料也不好找。

也许在特定平台环境（IDE）下能基本满足需要，但是拆开来单独用远不如makefile成气候。
所有这些DSL还有共同的问题：DSL到通用语言缺少过渡。

也有一些利用现有语言（语法和语义）的替代方案，减少从头设计DSL多出来的artifacts，比如用Ruby的Rake，用Haskell的Shake之类。

然而这些宿主语言在一定程度上本身就够麻烦了，所以整体上也不见得更简单……

考虑扩展也许这里用经过严格限制的Lisp方言的子集类似物作为基础最容易。

……以后再说吧。

PS2.图形库的轮子我基本没造（只是撸了几个光栅化以及替换了EGE的底层实现）。

我造的主要轮子之一正是native GUI的一般解决方案中强调需要去除体系结构和操作系统依赖的部分，而不是GDI+/D2D之类的图形库——为了简便起见还刻意弱化图形功能了（因为这是另一大坨适合单独实现的东西），硬件加速什么的也完全没实现。

声明式（布局）语言只是GUI的一部分，而剩下的部分即便不算图形也很麻烦。例如，实现一个浏览器的排版引擎，几乎就是实现大半个GUI框架（当然还有其它一些东西）。（顺便，我发现“尽可能分离UI和非UI”不总是必要的需求，在很多场景下导致不可能实现优化的效果，浏览器却在这里尤其受到影响而需要妥协——不过这些是另外的问题了。）

HTML设计一开始就没有考虑这类相对通用的目的，导致很多坑，即便没有W3C也会自己残——现在没js也就是残的（听说HTML6要去js化？）。

相对来说，XML以及XAML和QML之类的派生物就靠谱得多，可就是没有HTML+js这样有流行的多种实现。

ECMAScript现在大概也是朝通用目的而不是DSL设计了，这样倒正好腾出些位置。不过还有很多要取代js什么的跳出来……

可以预见，运行环境在很长一段时间内还是会相当混乱。

技术上来说，我不喜欢SGML派生物——要说通用文本序列化格式，不管是对机器还是对人，不管是时间空间效率还是实现复杂性/可靠性来说几乎都被S-expr等等完爆。即便只限存储、传输和编辑，也经常不如JSON之类。

只是因为历史包袱多，可用资源也多，所以才妥协罢了。所以不需要兼容，可以扔掉浏览器的场合，我绝逼不用。

退一步讲，浏览器的内部实现（不管是不是排版引擎）很多地方也满是洞，要接受这坨东西建立应用环境就得做好随时丢失可用性的准备，比如习惯吃内存太多就干掉浏览器进程。（V8分配不到内存就boom什么的……）

于是Web应用在本机环境上怎么说都是备胎（即便算上桌面应用嵌入Web页面的之类的情况），很长时间都不能指望太多了。

### 2015-04-13 00:02 Tippisum

1. 你看来还是挺闲的，有精力去教育逗比……

我这个人很多时候是话不投机半句多主义者……

3. 这里的问题主要还是心态。在网络上喷口水时候的心态和现实中交流的心态不是一回事。当然这方面的理论分析并不太多，所以更多的是我自己的一些经验。

4. 当我开始意识到，无论我付出多少努力，能控制的东西总归是有限的时候，我就很少去追求“完全控制”了。因此现在我的策略基本上可以归纳为“眼不见心不烦”和“人不犯我我不犯人”。只有当糟烂的东西给我带来了确实的麻烦，我才会考虑积极的去造轮子。

限度就是，不管怎么造，C++里的一些翔很多时候是没救的。再往下，操作系统里的翔超过了绝大多数正常人的能力。就算自己从操作系统开始撸，硬件架构的翔也终究还是要吃。所以在自己动手投入的精力和对翔的容忍度之间，总归还是得有个取舍。换我自己来说，大概就是我已经没有太多折腾的动力了……

PS1, Lisp的主要问题还是可读性差了点，所以用作面向非开发者用户的DSL的时候存在先天劣势。当然对于开发者用户来说也许这都不是个事儿，但只要现有技术可以满足需求，专门撸一套DSL出来的动力很多时候也不高。（后面提到的通用文本序列化问题也类似。）

此外，其实开发者用户也不见得都喜欢折腾。至少对我来说，为了编译一些看起来也不是什么大项目的东西，结果却动辄要在我的机器上安装数百MB的开发环境……这样的事情我也同样会觉得很烦。

PS2, 所以说主要问题说白了还是互操作搞不定……

所有native GUI库都普遍存在的问题，就是基本上它们都会绑架整个项目的接口。因此绝大多数开始看起来像是个GUI库的东西，到后来总是会逐渐变成臃肿的framework（又或者干脆一开始就是个臃肿的framework）。从这个角度来讲，虽然像SDL和GTK+这种东西看起来傻逼，但相对来说不怎么绑架程序接口这一点还是不得不承认的。

之所以要UI和实现解耦，很多时候也是基于这方面的考虑，而不纯粹是逻辑抽象的需求。因为GUI本身固有的复杂性和源自下层操作系统里的各种翔，互操作搞不定的话接口绑架可以说就是一种必然。强行要打破的话，只能从前后端强制分离等更加极端的办法里开始考虑——必须承认，这不一定就是合理的做法，有时候也是出于无奈。而回退到Web，则是最极端的一种解决方式。本质上讲，这和前面说的，“互操作搞不定的话就干脆不要互操作直接玩二进制序列化好了”，这样的想法是一脉相承的。于是自然的，在性能敏感的场合，这东西肯定是不合适的。

至于HTML，微软事实上很早就有想要用HTML来搞UI的想法。AJAX就是微软的其中一个尝试。另外早期的Windows里资源管理器的视图也是可以自己写HTML来自定义的。但Web的蛋糕大家都想霸占，各派势力互相撕逼，结果HTML就成了牺牲品。——总觉得某种意义上来说和C++的境遇有点类似。

JSON确实是在这一系列的混乱之中诞生的为数不多有一定价值的成果。兼顾可读性和实现的话，目前来看确实算是个让人还能接受的妥协。现在JSON越来越流行，也不是没有道理的。

### 2015-04-13 04:26 幻の上帝

1.因为逗比中有药救的不多，所以实际不怎么费时间。

4.控制也算是需求。如果Qt之类的能让我忍得下去改改用用就算了，我也不必要自己撸。（Qt5出来以后倒是比Qt4能忍一点，但还是比起我要用的差得远。）
总之，量力而为。

PS1.语法不顶用改起来也方便，剩下还是看设计。

作为DSL，和XML相比学习曲线不会有太大区别。

另外，这样正好容易使环境压力减少。

比如说，C++的编译器为什么这么复杂？一部分原因就是语法太抽风了。其它少有语言有这种复杂性，但很多也好不到哪里去。

XML直接一一映射成S-expr，体积少说小20%吧……

PS2.SDL和GTK+还不一样。SDL是功能一坨坨比较分散还算顶用，也有SDL_main这种糊上去的东西。

GTK+根本就是自己造了套对象系统的轮子了，不当framework比framework还麻烦。

这些东西虽然因为C实现的关系二进制互操作勉强通顺了，写到上层还是罗嗦。

这样还不如直接把framework做干净点，方便拆……

如果不是包装系统提供的native GUI，做干净还不算困难，只是工作量问题。

在这上面当然也可以过渡到Web（嘛，就是实现个浏览器了……），不过显然不必要。

AJAX一开始不是Google啥的搞的么。M$是HTA吧。

Web的根本问题在于：它一开始本来就不是为了什么做应用平台的技术。现在强行适应不是原本目的的需求，就得把原来不相干和不合时宜的设计抹掉，但因为兼容包袱又没法彻底清除（典型地，比如frame）。

浏览器越来越臃肿，自己就还自带一整套默认的笨重UI，还有各种兼容性问题，怎么看都做不好安分的运行时，还不如winrt什么的，虽然不见得就顶用但是好歹一开始就目的明确，没那么多妥协。

就算今后几年HTML/js能多更新几个大版本，考虑习惯，这些问题在相当长时间内还是难以解决的。

C++的情况比HTML好得多，至少新版本厂商都趋之若鹜一样没有不理的，更没有HTML5在Adobe支持下几年都甩不掉Flash这样的尴尬。
而且抛开历史包袱不谈，相当长一段时间内也找不到什么替代技术了。

### 2015-04-13 09:32 Tippisum

1. 同意C++的麻烦有一部分是抽风的语法带来的。

至于XML，虽然罗嗦了点，用还是能用的。而且有好使的文本编辑器的话，写起来往往也多敲不了几下键盘，冗余的文本deflate一下也差不多就OK了……总之，这玩意儿能用，带来的麻烦目前看来也没多到让大部分人不能忍的地步，所以可预见的将来肯定还是会广泛存在的。当然我个人肯定是支持用JSON之类更方便的东西。

2. AJAX的核心是XMLHTTPRequest，这玩意儿最早就是微软弄出来的。当然，微软使用了自家的ActiveX来实现，结果搞得其他厂家不爽，这中间又是一长串旷日持久的撕逼……最后的结果是微软认怂了，现在AJAX已经被标准化。

HTML的最大问题无疑就是它长期以来都定位不明。至于兼容包袱什么的恐怕还是个伪命题——只要W3C肯狠下心来把DOCTYPE Sniffing标准化，兼容性什么的根本就不是个事儿。反倒是js的兼容包袱可能还更大些。问题是W3C自己明显也搞不清自己到底打算把HTML当成什么。所以搞来搞去基本都是在乱搞……唔，从这个角度来说，其实应该说HTML的境遇更像C语言……

至于WinRT什么的，其实和Flash性质差不多。说白了，就是在HTML的基础上设计的合理能用点。问题就在于这些东西全都是私有技术，发展前景总归还是存在问题。——话说要是Adobe肯狠下心来把Flash给标准化了，我看有一堆东西都要被干死。可惜Adobe肯定不会这么做的。

### 2015-04-13 09:44 Tippisum

PS, 昨天给新项目添加使用Zopfli实现DEFLATE压缩的功能。重新认识到异常才是C++互操作里最大的一坨翔……其他的东西哪怕再怎么恶心，都还是有办法避免的，底线是可以自己造轮子。唯独异常这个东西是跟语言绑死的，一点办法都没有。

### 2015-04-13 17:27 幻の上帝

1.然而有时候开发环境就是不得不妥协……

主要槽点在于，一个技术上和各种替代品相比，怎么看都没有多出什么好处，发明年代又算不上多早的东西偏偏会流行呢。

仅仅是因为许多时候造成的麻烦不够明显和懒？恐怕不是，还是有相当多的无知用户顶着的关系。

嘛，这也不算个例……

2.Web的兼容性一直是个问题，而且恐怕会继续下去。

一个原因是W3C实际表现出来的对实现的“号召力”或者说有效约束太低，就算真的标准化了，厂商会立刻全部跟进么？就是厂商跟进了，开发者里有多少会马上买账么（看看HTML5 v. Flash）？

这点类似的还有ISO C。看看M$的C编译器对C99放置play几年了……Windows下搞C的用户似乎也对此不痛不痒，刺激M$的C99实现的居然还是C++11。

而ISO C++则相反，就是M$这样以前有一些很明显的扩展特性冲突的也口嫌体正直一概放弃/低调处理，最近几年反倒还在宣传上很起劲，虽然实现一直鸡肋。

究其原因，恐怕是C++过于复杂，不合作就会在核心上玩脱导致没法用，还有导致被弃用的风险（反正现在VC++在我这里基本就是这个地位了）。这点是不是很讽刺呢。

还有一个重要的原因：光是看用户数量，淘汰旧浏览器就比淘汰旧编译器什么的麻烦多了。

这里的运行时问题谁都没办法——又没法像msvcrt一样，不给装运行时就跑不动程序于是可以让用户自觉；或者M$说以后用Windows Update就没人能反对；也没法每个应用都随便附带一套。

js的历史包袱确实也成问题。不过就兼容问题，动态语言可以有些简单粗暴的解法，一坨if就能很方便地糊出一些#ifdef不方便搞定的东西。

虽然我实际上也很不喜欢WinRT这样限制明显的私有技术所以也没打算实际用，不过纯粹作为解决方案，比起历史包袱成堆的东西来说对有些开发者还是有些吸引力的。

Adobe不标准化Flash的原因是实现质量烂到自己也看不下去了么……？

PS.异常ABI确实非常疼，不过在有源码的情况下还能忍。这类ABI问题最让人抓狂的是没有源码的库之间不兼容。。。

### 2015-04-13 20:24 Tippisum

W3C的号召力应该也没那么差。微软跟W3C对着干了不短的时间，最后还是怂了。而且Flash也好WinRT也好多少也有吸收HTML5的一些东西，从这个意义上来讲也不算全是无用功。底线是至少lowest common denominator算是扩大了一点——不过也许真的只有一点。

ISO C是个特别令人吐槽无力的东西。主要问题在于那帮人到底想干啥呢？

要是觉得看不惯C++，明明还是有很多可以做的。就不说在吸取C++经验教训的基础上扩展语言特性什么的（其实我觉得可以有），就是把现有的东西给理理清，比如说把ABI之类的东西搞搞明白让大家都能有个坚实的基础，这也绝对能算是功德一件。

现在的情况是问题基本上都没怎么解决，坑的地方继续坑，技术上被C++拖着走但又总想挣扎……

私有技术的主要问题就是你也不知道哪天就会被坑了。当然用标准的东西该坑的地方一样坑（有时坑还会多些），但毕竟是摆在明面，情感上更容易接受一些。这东西还是看具体需求吧。

至于C++互操作，恐怕只能老老实实源码了。没源码的库之间兼容问题解决起来确实太痛苦。

### 2015-04-14 20:19 幻の上帝

W3C的号召力差是相对来说（VS2003以后的大部分时间内WG21的头子直接就是M$的native语言总负责人所以内部应该也没多少反对意见了，Web相关的工作似乎很长时间内都是热脸对冷屁股，没怎么见M$的影子），。这几年来看看上去是好了一些，不过前途嘛……

关于Flash，不是说就一定不顶用，但既然是已经注定放弃的东西，要死还是死得快一点吧，现在这种情况某种意义下更疼。

这里有一个问题在于W3C能控制一部分东西，而底下的实现是管不着的。当大部分情况下实现都不理想的时候，Web就有些在考验忍耐了：放弃现有的方便，还是本来能更好的用户体验？C++在这方面对应就好得多，即便未必现实：不爽就整个轮子栈造一遍（只有对于硬件不爽，那没办法）。

ISO C我感觉除了加一些dssq的东西（比如说C11的线程模型和atomic，wording还是从C++11里抄了不少……）就是修bug刷存在感了……论抽象C还是省省吧。
从一些细节上可以看出WG14确实比WG21更保守，这大概也还是为了兼容性。只是总体上更混乱了……

比如说C++11敢直接加static_assert，C11就迂回加上了保留标识符_Static_assert当关键字然后宏static_assert；再如同样加上了线程模型，C++11要求getenv线程安全，C11就不保证。
ABI固守现状我也觉得有些费解。明明C在这里比C++容易得多（没有name mangling什么的破事），可反而只是C++用户特别关心很显眼。也许真是因为C的ABI兼容性在某些实现上是事实标准，没有C++那么突出的问题所以不思进取了吧。

虽然被逼的情况下COM也许可行，C++的话确实是源码舒服多了……（特别是考虑M$VCRT静态库这种COM也不可能有药救的情况）。

### 2015-04-14 23:49 Tippisum

我觉得Flash在可预见的将来是不会死的，而且Adobe就算嘴上说放弃Flash，实际上真的放弃掉的可能性也不大。这玩意儿说白了也已经是尾大不掉了。能真正意义上干死Flash的只有HTML标准。但从HTML5的情况来看，我对W3C的工作效率极其没有期待。

至于C++自己造轮子栈，虽然已经有很久没造，但基本上大部分还是曾经折腾过的……
总之，大部分的翔都有办法，但还是有几个比较恶心的。现在能想起来的有一个thiscall和一个异常，都是能让人抓狂的东西。如果考虑到具体操作系统的层次，那还得接着吃几口操作系统的翔——不过比起其他大多数的语言来说已经要好得多了，这倒是不假。

之前说到js，有一点是js是动态语言，它没有明显的编译时和运行时的划分。何况#ifdef严格来说只是预处理，连编译时都算不上。
js用一坨if来实现兼容性，用肯定是能用，但代价自然也是有的。只不过很多时候当你开始用js的时候，这些代价基本上也就无所谓了。对于C++，要求肯定不一样。

其实这里也还有另一个问题，就是C++是一个极端的以编译性能换运行性能的语言——或者说已知的所有C++实现都采用了这样的策略，所以运行时动态生成C++代码从实践上来看几乎是不可能的。

比如说C#的泛型确实能力严格弱于C++模板，但它还是能做到一件C++做不到（或者说不现实）的事情，那就是运行时特化……

其实没可能有多少人真的指望C能表达高级抽象。说白了C还是起到一个least common denominator的作用，给互操作留下点最后的空间。所以如果ISO C真的明智的话，就应该多在这方面下些功夫，想办法把这个least common denominator尽可能扩大一点才对。至于那些琐碎的语言特性还是交给有精力去折腾的人吧。

至于M$VCRT……我只能说这东西就是个傻逼……

尤其是当M$不知道吃错什么药开始弄出一系列的M$VCRXX之后……（虽然我能理解M$的无奈，但还是无法接受这个。）

经常看到一些软件的二进制包里面带着好几个不同的M$VCRXX，然后就深刻的感受到C++的互操作性真是一坨翔，而M$显然就是个在翔上加料的搅屎棍……

### 2015-04-15 02:00 乐天派的小耗子
捏捏。

### 2015-04-15 02:03 乐天派的小耗子

居然没人提我大 mcf。shared_ptr 污染接口的问题基本没法解决了，最后还得自己造 intrusive ptr，那么问题来了，weak ptr 怎么办？

### 2015-04-15 02:09 乐天派的小耗子

至于 msvcrt 的问题么，糊个 mcfcrt 静态吃掉 libstdc++ 然后一起 lgpl 丢外面就是，msvcrt 还是有异常依赖 sprintf 啥的没办法，不过还好不是带数字的。

### 2015-04-15 11:23 Tippisum

shared_ptr的问题在有了unique_ptr之后已经好多了（C++11之前shared_ptr泛滥的一个主要原因还是auto_ptr根本不能用）。现在的主要矛盾是各类容器。

### 2015-04-15 11:37 Tippisum

至于CRT兼容性的那坨翔我都不想提。如果CRT兼容性有需求，我现在的策略是干脆直接放弃msvcrt。反正标准C库里的函数不少都能直接导入ntdll。operator new之类的也能直接扔HeapAlloc（其实msvcrt自己也是这么干的）。

### 2015-04-15 16:11 幻の上帝

回复 乐天派的小耗子 : 所以我说你没法强制别人用那么基本就没戏。退一万步讲，就算你的吱运行库像M$VCRT那么流行了，你也干不掉渣玩意儿静态链接烂库。

### 2015-04-15 16:12 幻の上帝

回复 Tippisum :嘛我现在自己的东西就只管mingw用死旧msvcrt了，上面的dll都自己分发，管它什么发布策略——这样虽然疼蛋兼容应该没多少问题。

### 2015-04-15 18:45 乐天派的小耗子

回复 @Tippisum :ntdll 里可没有 sprintf。

### 2015-04-15 19:16 Tippisum

回复 乐天派的小耗子 :有snprintf和swprintf，够用了。真不行还有kernel32.dll里的wsprintfA。

### 2015-04-15 20:53 乐天派的小耗子

回复 Tippisum : M$ 那个 swprintf 根本就不兼容 c99 （比如 %s 按 C99 就是 a pointer to an array of character type。M$ 那个虽然情有可原，但是免不了坑人）。至于 snprintf……您说的是 _snprintf 么，我用 VC 的时候微软从来就没支持过 snprintf。

### 2015-04-15 20:58 乐天派的小耗子

回复 幻の上帝 :我持类似的观点。目今 libgcc 和 libstdc++ 是依赖了 sprintf 一类杂货导致脱不了 msvcrt 的干系了（虽然 libmingwex 提供了 public domain 版本可以用不过还是得重编一遍，因为其实不是一个函数。或者用 -Wl,--wrap= 改掉 symbol 然后重定向到 __mingw_sprintf ——未测试）。

### 2015-04-15 21:00 Tippisum
回复 乐天派的小耗子 :你说的对，是_snprintf。确实坑还是有，不过小心点用勉强也够了。

### 2015-04-15 21:00 乐天派的小耗子

回复 幻の上帝 :C 的破烂货太多，比如 strtok strcoll 啥的。这类东西兼容是包袱，不兼容还不符合标准——反正我是不指望这东西，要用就自己实现去。常用的 math stdlib 一类稍微实现几个（尤其需要关照的是内存管理、memcpy memchr strlen 一类）就能解决不少问题了。有些东西干脆就弄成扩展（待续）

### 2015-04-15 21:01 乐天派的小耗子

回复 幻の上帝 :比如接口类似于 std::copy 的 mcf_strcpyout （和 MySQL 的 strmov 是一样的功能）——显然 strcpy 和 strcat 这种东西是无必要支持的，干脆扔掉算了。

### 2015-04-15 21:06 乐天派的小耗子

回复 Tippisum :关于 unique_ptr：c++98 环境下依赖编译器 RVO 干掉有声明无定义的拷贝构造函数是有解的——默认初始化一个对象放在栈上，然后用 ADL 调用 swap，再返回自动变量——至少 gcc 是无视优化级别完全干掉这个拷贝调用的，（待续）

### 2015-04-15 21:07 乐天派的小耗子

回复 Tippisum :用这种办法在函数形参以及返回值上直接传 boost::scoped_ptr 是可以的，但是依然不能放在容器里。考 [url]https://github.com/lhmouse/poseidon/blob/master/src/main/cxx_ver.hpp#L99 [/url]

### 2015-04-16 13:26 幻の上帝

回复 乐天派的小耗子 :*printf/math里的一些东西/stdlib的一些玩意儿懒得造暂且除外，剩下的这些libc破烂货我自己不用，用了的是标准库实现。

### 2015-04-16 13:27 幻の上帝

回复 乐天派的小耗子 :接口类似于std::copy有个疼的地方，dest和src纠结完哪个在前面了么。。。

### 2015-04-16 13:28 幻の上帝

回复 乐天派的小耗子 :嘛，没法放在容器里这点不用unique_ptr要解决看来就是得搞一套AA的mojo类似物了，太疼。

### 2015-04-16 13:30 乐天派的小耗子
回复 幻の上帝 :dst 在前面。

### 2015-04-15 16:09 幻の上帝

Flash……大势所趋没办法，毕竟主要厂商姿态都定型了。而且毕竟没C这种关键领域的应用（如操作系统内核、硬件驱动程序、版本控制系统）的不可替代的依赖性。

当然死成什么样子不好说。Turbo C某种意义上也不是没死么（呵呵）。

从技术上来说C++没太大的局限性，除了硬件拙计到C也几乎没法顶用的场合。主要还是现实理由：需要持续投入、搞起来工作量太大、会维护的人难找。

C++最大两块——编译器和操作系统内核——的轮子自己造完整没现实性（认命吧这里是得继续吃翔），但上层那么点要能用，还是有点戏的，虽然不会是人人都会。得花不少时间而且不能指望推广成业界事实标准就是了。

`#ifdef`的语义是明确体现在phase of translation里的，确实不一定是“编译时”但是也类似。而且逻辑上preprocessing translation unit→translation unit的“编译”过程比后面真正的翻译还清楚地指定了，去解释实现几乎就是没事找事。

然而运行时的要求确实不一样。顺便提一下，老实说，即便“运行时”是什么意思在C++这样的语言里根本没spec，我原则上不太喜欢这种要求。极端情况下这种要求会逼用户把相同意图的代码用不同的姿势写几遍：`const int i = 42;`/`using i = std::integral_constant<int, 42>;`。`constexpr`支持半吊子partial evaluation某种意义上模糊了这个界限，算是一个小的进步。

所以我觉得除非是DSL，以后会是能随意按需静态化的动态语言才是王道。当这里的问题搞清楚以后，“极端的以编译性能换运行性能的语言”也不会成问题，而是可以根据用户需要调整侧重点。
关于通过元数据反射，C++也能做到，就是不自带、没标准化、需要人肉写很多冤枉代码，所以基本不能实用罢了。说C#严格弱于C++也是不对的，一个比较明显的硬伤是variadic，而C#的lambda缺乏类型导致一些地方写起来远远比C艹麻烦，这些应该算是一厢情愿的设计失误。

而真正意义上体现“动态”特性的东西如evaluation tower，几乎没什么语言实现能做得好，C-like的静态语言就别说了，C#加上dynamic也得靠另外糊DLR，原生机制根本不够用。

C确实不被指望能表达高级抽象，但是C的发展和规范本身体现出来的也未必就对“底层”友好——C++对实际上的底层的限制还更严格清楚一点，比如指针关系操作的全序，C就没有。当然C这里未必就设计得更烂，只是要说在这个意义上说C“接近底层”，实在有些那啥。

如果真的只是要least common denominator，还不如C--这种拿来实现高级语言的IL，对体系结构有一定的兼容性，对实际实现的抽象也可能更合理。

但实际情况是，除开互操作，C被普遍用于实现一些基本应用——换句话说，写操作系统内核用C是能在一定程度上比C++偷懒没错，但DBMS、VCS这类应用还是大都用C写底层，我只能说太屈从于历史包袱了。要知道后者就是有互操作也几乎是高层意义上的，拿个解释型语言当DSL用然后糊上boost.python/swig啥的也并不是有什么绕不过去的困难（虽然确实麻烦，但不是lua这种直接内嵌，C也没好哪去）。

据Mr.STL等说M$VCRT傻逼情况从VS2015 RTM将会被另一个逗比——Windows Update缓解。
我遇到的一个真实sb案例还不能全说是M$VCRT的错——发布的没源码的静态库居然他喵的静态链接了libcmt，用到了msvcp却根本就没法换版本，于是为了项目需要支持不同版本的库就要装不同版本的VS，每次提交前测试构建就得在卡翔的开发机一遍遍打开各种VS，我！@#￥%……（嘛，我平时不爽大部分闭源软件，也有这样的原因。）后来我怒艹nmake，终于可以在发布前只打开一个VS构建了，然而没单独工具链开发机还是得装不同版本的VS，总之还是非常不爽。

### 2015-04-15 16:29 幻の上帝

@Tippisum ,@乐天派的小耗子 :
shared_ptr算是个案 ，但是更一般意义上的接口污染还是有些其它槽点的。一个可能显著的现实问题是这种接口污染可能造成一些兼容问题。

源码上来说，相对容易解决：造个基本同名的轮子即可，简单示意就是这样：

```
namespace my_app
{
using std::shared_ptr;
//using boost::shared_ptr;之类，也可以用自己的类似轮子。要说命名风格冲突……看着办，你能完全不用std的东西而像M$一样做到typedef void VOID;的程度么。还有个坑就是这些轮子之间一些情况下在源码上都不保证完全兼容，这就要看各人跳坑能力了。
}
```

然后睁一只眼闭一只眼把shared_ptr当“关键字”用，剩下的也不管了。

二进制除了再套一层inline namespace减少版本版本变更的风险好像就真没什么办法了，不过这种东西直接取符号本来也是自找没趣。

而如果不说兼容性，C艹的情况某种意义还好一点：起码能让你选shared_ptr还是unique_ptr还是raw ptr之类，看看Java什么的，直接就内建语言捆死在GC上了，逼用户排除本来能实现得好好的需求——来换取表面上看起来的干净。

这种比下有余的情况下，兼容性和接口污染的问题还真不一定就是什么大问题了。

### 2015-04-15 16:31 幻の上帝

嘛，Java这里也不是一点都没东西能用，不过java.lang.ref似乎正常Java码农都不太会去注意里面有些什么。

### 2015-04-15 18:36 Tippisum

Flash很难再有什么大的发展，但死肯定是不会很快死掉的。而且其实Flash在很多场合还是很有用的（我经常在各种场合看到用Flash来做UI，甚至有比如说像the Elder Scroll这种大型3D游戏也在用）

C++不得不吃的翔基本上就是编译器和操作系统了。其他的反正理论上讲怒造轮子都是可以的。
我同意编译时和运行时的区别不是绝对的，而且能够直接对语言本身静态化而不是刻意区分编译期和运行期也确实有潜力（C++的模板元编程还是略疼，constexpr也不总是顶用）。

不考虑翔一样的互操作性的话，运行时对象创建和接口查询之类的基本反射C++肯定是能做的，甚至现成的库也有。但模板运行时实例化基本上是不现实的，就算能做出来，考虑到效率和蛋疼程度估计也不会有太大的吸引力……（C#泛型对值类型的特化基本上还是要靠运行时JIT支持才有的玩，在iOS之类的设备上实现的时候就疼的要命）

C#也是一开始没有考虑到模板，后来才有了泛型。不过好歹微软还是没判断错局势，把泛型支持搞到CLR里面，从实用角度来说是个质的提升。Java这方面真是反面典型。为了所谓的兼容性，把自己最大的优势——互操作性给丢了。Java里涉及到泛型的互操作真是一个不小心就掉坑，比之C++有过之而无不及。

至于你说的情况，我只能说静态链接libcmt简直是会玩的……

shared_ptr，反正就连我这个不喜欢造轮子的人，造轮子列表里还是常年有这东西，真的是槽点太多了，令人印象深刻。有了unique_ptr之后已经好多了，不过unique_ptr的轮子有时也还是要忍不住自己弄一个，毕竟这东西几乎到处都会用到，能把性能稍微提升一点的话还是值得的……

### 2015-04-15 20:55 乐天派的小耗子

造个支持构造函数里面 shared_from_this 的 shared_ptr 或者 weak_intrusive_ptr 吧。

### 2015-04-15 21:02 Tippisum

回复 乐天派的小耗子 :现在基本不用shared_ptr了。感觉大部分时候unique_ptr以及自己造的山寨货已经可以满足需求……

### 2015-04-15 21:46 乐天派的小耗子

回复 @Tippisum :unique_ptr 个人觉得没必要单造。我造这东西只是为了好玩。

### 2015-04-16 13:13 幻の上帝

回复 乐天派的小耗子 :在实现普遍没法把shared_ptr随便脑补扔掉control block以及考虑ABI（嘛，就是sizeof）的情况下用shared_ptr显然不现实。没unique_ptr你靠什么干掉auto_ptr呢，难道还退回去用builtin ptr？

### 2015-04-16 13:29 乐天派的小耗子

回复 幻の上帝 :用标准库的足够，无必要自己造。

### 2015-04-16 13:10 幻の上帝

@Tippisum .

C++的TMP对一般应用是略疼，对就该用元编程的地方（比如embedded DSL）是相当疼。考虑到其它大部分主流语言甚至连实现的现实性都没有，所以这里忍了。

constexpr不顶用的地方在于：

1.相对于非constexpr，技术上它必须暴露实现（函数体，或者初始化的值）。不过这个是相对而言，也不是大问题（其它大部分语言甚至都不见得有提供函数级别的接口和实现分离，非constexpr函数和extern变量可以算是C++的bonus）。

2.基本机制上的半吊子。考虑到实现的复杂性，constexpr的规则在检查出问题这点上不是强制的，可以明明ill-formed就是no diagnostic required。这个导致有些地方很不靠谱，写错了都可能不知道，只能看实现高兴（G++就经常这样，Clang++这里报错就好得多）。

3.语义上的无能。constexpr要求literal type，这个太严格了。不过谁叫C艹的语义已经够乱了，不这样就说不清楚了呢。

比没有当然还是要好一点。不过我希望的是取代原来的麻烦的东西，然而这个进步又太慢了——就是因为乱七八糟的细节太多了，没法指望。

一个相关的例子：

integral_constant在TR1基本就是用::value取值；C++11有constexpr可以用constexpr operator value_type()了就比较方便，很多情况可以把::value直接替换成()，但不是明确需要value_type的地方还得自己转换；C++14又加入了constexpr value_type operator()()，直接构造就不管转换了更加方便。

（M$VC这逗到VS2015 RTM支持全constexpr都没戏，Mr.STL说在此之前不会改库实现所以连C++11的都别想了，于是最近彻底扔掉了M$VC……）

上面说“有些地方”能用转换操作符，是指C++11模板实参中没法用，这样就迫使要么都::value，要么有::value和()/{}()两者共存的不直观的情况。于是C++14后在这里还做了修正……
像这个例子里的这些东西，要“一开始考虑”彻底基本是不可能的。根本上解决这种问题，是核心语言特性设计时就必须考虑清楚的。C++在这里犯了一个根本性的错误：模板脱离类型系统之外使用了过多的单独的语义规则，导致库写起来有冗余，核心特性改起来又各种麻烦。

相比之下，没有元数据支持这种问题的范围就确定得多——涉及的用户没那么多，而特性没有也能自己造库来实现，虽然恶心，但是不至于像上面那样非得依赖于核心特性，甚至自己造方言和编译器。对于特定的一般（不会自己实现的）用户来说，仍然一样麻烦是了。

关于这个问题，我觉得C++和C#乃至现在几乎所有语言都是闭门造车。正确的姿势应该是一开始就向用户（库的作者）提供扩展重写系统的机制，而类型系统建立在重写系统的规则之上。模板这种东西的核心规则不应该那么复杂，包括参数类型、依赖类型这些常规语言的特性或者和语言实现以外的互操作的保证等，都应该让（系统）库的作者实现，一般用户使用这些库完成任务。
当然，语言实现内部的互操作性方面对语言的设计者要求会很高——要提前预知清楚哪些东西是可扩展的，在编译器和运行时给库的作者留下接口。但无论怎么说，在同时保留现在C++和C#类似特性的前提下，扩展和兼容问题上不会像现在那么被动。

这种的设计实际上有更广泛的意义——让一种通用的语言通过加上特定的库，自然“退化”到另一种更具体的语言：加上某个库就是C++的实现，另一个库就是C#的实现——因为肯定会公用（比CLR更一般的）IR，二进制以上的互操作性根本就不会是问题。现实最接近这种角色的是抽象的LISP，然而Lisp的各种现有方言仍然主要和其它语言竞争来解决具体问题，并没有认清这里的定位，这里的玩意儿长期没有进展，更没有标准化中间层技术到现实能用。如果我要造语言，迟早要把这块补上（向用户证明可能性，是不是真的值得要我亲自实现另当别论）。

至于Java前无古人后无来者的逗比设计……恐怕都习惯了，不管是不是厨都觉得这个尿性很正常。
故意链接libcmt这个我之前完全想不通是什么动机，特别是现在默认都普遍msvcrt，要想出这茬也得把天赋点到整人上。我猜只是因为更早的旧版就用的libcmt？

标准库有的东西，如果不是明显不能满足需求，我都忍了不造轮子，即便是造也尽量不改变接口，原因：

1.标准库不会整个扔掉，自己造的轮子的交互毕竟麻烦。就是__gnu_cxx::__versa_string比起std::basic_string都是被歧视的（不能用于stringstream什么的），尽管在libstdc++中前者更能符合ISO C++对basic_string的要求。

2.标准库的spec是有一大票人系统性维护的，我一个人要造各种各样的轮子而不是Mr.STL、J.Wakely、H.Hinnant这样专艹标准库，没那么空保证实现以上的什么defect都不漏下而自己写spec。所以这里即便造轮子自己实现，我宁可直接复用ISO C++的接口而不是抄一个类似的。

3.第三方使用标准库的用户是多数不特定群体。如果自己提供接口，要复用这些用户的代码（虽然大部分是渣但能用的绝对数量还是不少）显然就是给自己增加工作量，最后变成为造轮子而不得不造轮子，太浪费时间。

### 2015-04-21 11:16 cqwrteur

@幻の上帝 经常听到有人鼓吹说"大多数人自己无法管理好内存，GC做的比他们都好。至于网络文件这类的资源，都不会让这类程序员去写。"

不知道你怎么看。

### 2015-4-21 14:09 幻の上帝

也许还真是大多数。这类小白用户不配进入专业撸码领域，速速滚粗。

### 2015-04-21 13:07 Tippisum

突然发现VC++可以用__thiscall来声明函数指针，把this当成第一个参数就可以了。

终于可以不用翻来覆去折腾成员函数指针那坨翔了，突然感觉生活美好了不少……

### 2015-4-21 14:10 幻の上帝

__thiscall也就是实现在二进制上开个洞而已，和thunk什么的一概没事不碰。

### 2015-4-21 19:50 Tippisum

回复 幻の上帝 :我当然知道成员函数指针有多少坑……但很多时候我需要的真的只是那个洞，能让我不要去写一坨又臭又长的宏……

### 2015-4-21 21:12 幻の上帝

回复 Tippisum :一般用std::function就够了没必要乱折腾这个。二进制肯定坑多。

### 2015-4-21 22:07 Tippisum

回复 幻の上帝 :会有些非正常需求，比如说折腾一些Reverse Engineering之类的东西。有源代码的情况下本来也就犯不着捣鼓二进制。

### 2015-4-22 09:00 Tippisum

回复 幻の上帝 :底线是，指针算术可以自己做，但调用约定在C/C++语言的层次上是没有办法的。如果编译器不肯开洞，那只能要么上汇编，要么发明一些奇怪的宏……所以说C++的二进制说多了都是泪……（当然，仅限公开或者半公开的接口。纯私有函数编译器没有义务使用标准的约定，该上汇编的时候还是得上）

### 2015-4-22 12:07 幻の上帝

回复 Tippisum :那就没办法了……老实折腾吧。反过来说，有些人还有“难破解”的需求呢，这方面就正好是C艹>C>>Java啥的了。

### 2015-4-22 16:29 Tippisum

回复 幻の上帝 :Reverse Engineering不一定等于破解……对我来说，很多时候是为了对付一些难搞的程序，比如说明明是必要的扩展功能，却不老老实实导出符号……
### 2015-4-22 16:32 Tippisum

回复 幻の上帝 :真正搞破解的时候反而顾虑没那么多——反正破解这东西只要能用就好了，性能很多时候是无所谓的。有时候懒得专门开一个项目写注入代码，直接调试器脚本走起这种事情也干过。但如果是要正经来用的话，肯定接口是要搞好的。

### 2015-4-22 16:37 Tippisum

回复 Tippisum :有了__thiscall就能把那些成员函数导出成C风格的接口。之前一直是用union hack，加上一坨DEFINE_METHOD，CALL_METHOD之类的宏……特别难看……

### 2015-4-22 16:38 幻の上帝

回复 Tippisum :Reverse Engineering当然不见得是破解，不过有Forwarding Engineering的自己控制的项目一般也没必要这么搞的吧。这也就是hack了……

### 2015-4-22 18:49 Tippisum

回复 幻の上帝 :让我自己搞我肯定不会这么搞啦。只有当弄不到源代码或者程序太大依赖太多实在不想自己动手去从头编译的时候……不过现在自从开始在虚拟机里部署开发环境之后这样的场合已经少多了。反正不管是多么脏的开发环境也可以搭的起来……

## 补充材料评论 2015-05-07 17:27

> One 语言具有 first-class function。函数可以作为值任意传递。跟 Scheme 一样，这是一种真正的 lambda。很多语言虽然有叫 lambda 的东西，但却不能正确的实现，比如 Python 和 C++11。

> 具有真正的 lambda 的语言都需要 closure optimization。这个优化如果不做，就会产生大量的内存访问。C++11 要求程序员自己写出 free variable 在 [...] 里，就是因为它的编译器不能做好这个优化。在 Chez Scheme 和 Kent 的课程里，这种优化都是做得很好的，做到了极点。

...

> 不区分stack和heap变量

> 跟 C++ 和 C# 不同，但是跟 Scheme 和 Java 类似， 所有的变量都只是一个名字，程序员没必要知道它被放在堆栈（stack）上还是堆（heap）上。做这样的选择是为了让“名字”这个概念更加有一致性。为什么程序员需要知道它在哪里呢？这种事情本来就应该是编译器来做的。程序员所要知道的只是“一个名字对应一个对象”。

> 编译器尽量把对象放在堆栈上，这样可以减少 heap 的碎片现象。

——王垠《one语言的设计理念》

专门挂婊。

其实这篇文章本身倒和GC没多少关系（作者在后文中倒是表示考虑实时性现时不考虑使用GC，虽然我记得实时性问题在GC会议上多少也有被研究过，一个结论是并没有根本矛盾），挂的是一种具有普遍性和代表性的、显然错误大多数当事人却不自知的观点。

总结起来还是老调重弹——无知+瞎YY用户需求，嘛……在分析这点先说引文本身——为了表现错误的简便，先说第二段。

1.所谓stack和heap变量的区别，在具体语言中是否有所区分的情况不全相同，这点没错。
但是，名义上的区别和实质上的区别也不是一回事。例如，C++本来就没所谓stack和heap变量的说法。

具体到C和C++（搬出C是因为C的设计在这里显著影响了C++，而且C同样被普遍地、类似地误解，尽管ISO C甚至就没什么“变量”的概念），猜一下比较接近的意思，需要考虑的至少有两点：C对象、C++对象和引用的生存期；C和C++对象的存储期。

两者完全是相关但不同的抽象。而所谓stack或者heap，根本就是实现细节。稍微沾边的正式说法是，C里面叫自动/分配生存期/存储类，C++该叫自动/动态（剩余操作接口细节不一样的地方略）。注意，这是对象（以及C++引用）自身的属性。而所谓stack和heap根本就是不同的东西——在C里就没有，在C++里也就是后者用来实现free storage。

所以说所谓stack和heap变量的说法至少在一些语言里本来就是荒谬的：既混淆了接口和实现；又混淆了不同层次的抽象。而其它严肃的语言也并不敢直接在规范里这样扯蛋，非要说也是捆上运行时（而不只是语言本身）的规范才敢，否则就说不清楚。

另一个槽点是，ISO C和ISO C++是无所谓什么stack或者heap，JLS反倒分清楚了。（有JVMS唯恐天下不乱？Dalvik喝西北风是吧。）考虑到StackOverflowError，倒还真不敢故意跳过不碰stack（虽然敢语焉不详）。相对地，因为带有明确的用户可控的副作用（导致显式的程序行为），C++即便要说清楚stack unwinding是啥不碰stack都绰绰有余。哪种设计加重用户的负担呢。

对于这样一个错漏的概念照用不误然后一本正经批判，吃饱了没事撑着的。

2.“所有的变量都只是一个名字……程序员所要知道的只是‘一个名字对应一个对象’”——至少在通用语言中这实际上除了能说明了“变量”这个概念的废柴，并没有什么卵用。

这里实质蕴含一个陈述：用户只需要知道名字就能让“变量”顶用，即便不知道所谓的“变量”是什么、从哪里来，该到哪去，乃至是不是必要。

作者在这里暗示，编译器（语言实现）应当能够完全代替用户，了解变量放在哪。了解变量放在哪的理由就是“减少 heap 的碎片现象”。

有点常识就知道这是笑话。（不少Java用户和GC厨在这里没常识倒是真的。）

了解变量放在哪本身并不是什么目的。知道变量“放在哪”实质上是“怎么放”的一个副产品（上面也说了“放在哪”是另一层次的抽象，对用户而不是实现者来说许多时候根本无所谓）。

至于知道“怎么放”，为的是使用户能够按需控制程序符合预期的行为，这包含以下几个方面：

(1)清晰的边界和逻辑：在程序的哪些片段中变量是可用的，而另外一些时候这些变量是不需要考虑的，以使程序的逻辑更清晰；

(2)更明确地指示语言实现允许优化的外延；

(3)简化语言实现需要实现的优化逻辑，同时获得较好的翻译时性能和运行时性能。

光考虑下面两点，这些需求现实能被编译器很好地实现了么？显然没有。否则，C#抄了那么多Java特性却为什么在这里重新捡起来Java扔掉的区别呢？

只是因为C#的编译器写得烂？即使没常识也应该知道事情不是那么简单。

要编译器代替用户了解放在哪显然是不够的，也没多少直接的用途。对于语言实现来讲，更重要的问题是时机：什么时候引入一个变量（的存储），什么时候释放。

如果用户在抽象上对存储策略进行分类，那么他们就可以相对精确地控制（至少能做到保证相对顺序）。编译器在这里能做的都是硬编码的逻辑，也就是所谓的启发式策略，例如分代GC依赖的假设。

在什么时候最合适这个问题上，一个不明白精确规格的非强AI程序永远打不过一个了解精确需求和正确实现姿势的人类，更不可能方便地调校策略。这就是GC之类偷懒方法的一个根本缺陷。

若用户没有能力或者不被语言设计者允许表达不同的资源管理策略，任何形式的自动优化同样叒逼。

在可预见的未来内，强AI或完全形式化设计和验证的现实不可操作性注定了通用目的语言上的设计在方向总是错的。还想再来一次AI Winter？2young2simple。

更别说不能很好地满足(1)给真正想理解清楚需求的用户造成了更大的麻烦。

这怎么看都是鼓励无知的反智主义，最可能的起源正是作者对现实需求的无知。

3.“很多语言虽然有叫 lambda 的东西，但却不能正确的实现，比如 Python 和 C++11。”
撇开GvR的强迫症不讲（黑得好），这里所谓的不“正确”并不是什么实现，而是设计本身。

C++的问题非常明显，总结起来一句话：closure type非一等类型。（当然，光说C++11还有一个就是残废——比如参数多态的“泛型”lambda和capture list里的initializer因为进度问题，到C++14才填坑。）

反过来说，真正的“实现”（比如，closure type使用其它已有语言中的概念的定义），C++在这里简直做得没法更伟光正：不让用户做多余的假设、照顾了实现可行性、给实现足够的余地进行优化。

（反倒是C++11以后CWG的修正不够意思：要求显式C++ linkage导致没法做signal handler，以及禁止作为literal type简化实现和减少用户的潜在错误假设却妨碍优化。）

这段别的地方看来也没说语言的实现（只说lambda的“实现”），说来说去都是语言必须要求的“设计”，否则一坨说不通（或者只能体现对语言实现的外延缺乏概念）。

4.“具有真正的 lambda 的语言都需要 closure optimization。这个优化如果不做，就会产生大量的内存访问。C++11 要求程序员自己写出 free variable 在 [...] 里，就是因为它的编译器不能做好这个优化。在 Chez Scheme 和 Kent 的课程里，这种优化都是做得很好的，做到了极点。”

closure optimization没说清楚是啥，大概是指以存在closure为前提的一类程序变换效果的统称。

这里就有几个先入为主的低级槽点：

(1)谁要求lambda必须就实现成closure的？只要不是ABI有要求（明显不是这里C++的情况），没有capture的free variable的lambda很容易优化成一个function pointer，根本就没这种“不优化就会产生大量内存访问”里要“访问”的东西。

(2)谁保证不做程序变换之类的optimization就必须蠢到有“大量”内存访问？话说回来，非要说受到这类optimization可能影响的“大量”，只可能是不限制捕获范围的情况。像C++的explicit capture list反而能最大情况下减少“大量”的可能性。

退一步讲，真要不限制具体捕获什么变量，还有`[=]`和`[&]`，反而更加精确自由。不过靠这种程度的扯蛋智商可能也没法理解这里的区别了，我也懒得展开了。

然后是一个关键问题——显然语言规范不需要也不应该指明这类optimization的外延。至于不做是实现烂，纵容不做是用户智商低，能关语言设计毛线？

退一步讲，真说实现——没做？还是说，为了让优化更能发挥作用，就应该纵容用户写稀里糊涂的代码？

“就是因为它的编译器不能做好这个优化”这种大言不惭的笑话也看不清是怎么脑补出来的。
话说，发挥成这样，这里没顺带噗let expression也太可惜了吧。

###　2015-05-07 17:28 幻の上帝
ref:http://tieba.baidu.com/p/2411685175

## 引用回复 2

### 2015-08-02 00:12 Tippisum

话说关于这个问题又有了一些想法。

首先还是先明确下讨论范围。

自动资源管理 vs 手动资源管理 / deterministic vs indeterministic.

虽然都跟 GC 有关但实际上还是有很大不同的。

自动管理跟手动管理这个问题，以一个比较极端的方式来考虑的话——其实自动管理在很多时候都是强制的，你没法摆脱它。

这最终还是要取决于如何看待“自动管理”和“手动管理”的差别和界限。但毫无疑问，任何现代计算机系统上的资源——没错，任何资源，它们一定在（比我们讨论的“程序”要低的）某个层次上是受到自动管理的。

你当然可以写一个 C 程序，申请内存从不释放，打开文件从不关闭。但只要程序一结束，世界依然清净。你真的“手动管理”了这些资源么？从某种意义上，是的。但资源的所有权实际上仍然被操作系统掌控。

运行在现代计算机系统上的程序，无论用何种语言写成，它们一定是在操作系统和运行时所严格划定的圈子里面执行，绝不能越雷池半步。它们所使用的一切“资源”，也都是由操作系统和运行时控制的。无论有没有本文语境里的那个“GC”（或者范围放大点说“强制的自动内存管理”），这个事实都不会改变。有差别的地方无外也就是圈子大一点和小一点的差别罢了。（有例外，但不在这里讨论的范围内）

话说既然我们讨论“资源”，倒是有一种特殊的“资源”，对程序的执行至关重要，但在（程序语言设计这个框架内）却基本上没有人讨论。甚至很少在此类语境下被当成“资源”来讨论。
那就是

「CPU 时间」。
“在现代计算机系统上，几乎永远受到强制的自动管理的几种资源之一”

这个东西实在是太理所当然了所以很多时候是根本意识不到的。但 CPU 调度确实是个很大的课题。而且抢占式线程模型当然也存在各种缺点和不便之处。直接或间接与之相关的各种改进，如协程 / 异步等等也都是现在比较受关注的问题。最近的操作系统，为了更好的提高某些服务程序的吞吐量，也允许进程本身有限度的参与调度过程。

然而根本的图像上并没有改变。作为每个程序执行所最根本的“CPU 时间”这个东西，程序本身是没有能力去进行管理的，当然也没有语言提供这样的机制。协程的存在使得「有限度」的管理成为可能——说“有限度”是因为它只能提供转移分配的能力，申请和释放这样的正规资源语义当然是out of the question。

之所以会变成这样，一个很简单的理由还是，这个资源实在是太重要，也太敏感了。一个程序拒绝交出 CPU 时间，就可以直接挂起整个系统。这可是比泄露个把内存或者文件描述符什么的要不知道严重多少倍的事情。

The Good old days，Windows 3.x 就是一个协同式多任务的操作系统。尽管基于窗口的 CPU 协同调度设计精巧，而且大部分时候（令人惊讶的）工作的不错（以当时的眼光看来）。但还是没有办法改变只要一个程序出了点差错整个系统都会跟着完蛋的事实。所以当支持基于硬件的权限控制的处理器推出的时候，操作系统（毫不令人惊讶的）立即实现了抢占式调度，彻底将这个资源完全掌控在手中。

相比之下，内存调度这个东西之所以这么容易被玩坏，归根结底还是内存这东西确实没有 CPU 重要。应用程序到底是手工管理内存，还是自动管理内存，抑或是干脆不管理内存，在如今大多数的场景下确实没有什么大不了的。只不过这样的现实毫无疑问助长了不合格以及不负责任的程序员写出垃圾程序。

从这个角度来说，GC 并没有什么错，它的存在是为了解决问题。而且它关于“内存相比于 CPU 而言是更贱的资源”这样的假设，虽然不爽，但无疑也是事实——至少一定程度上是，否则的话打从一开始就不会有 GC 出现的必要性了。

但在某种意义上，也许这个问题不解决也好。

由此引申出的另一个点是资源管理有两个问题，效率问题和安全问题。这个之前也讨论过。只和效率问题有关的语境下，允许手动管理（或至少是有限的手动管理）当然是更加合理的选择。不过一旦牵扯到安全语境，强制策略几乎总是必须的。操作系统必须强制的自动管理 CPU 资源，并不是因为抢占式调度器总是效率更高，而是因为如果它不这么做，任何一个程序的 BUG 都会严重影响到整个系统。显然，总会有程序以为自己能料理好自己，但实际上却并非如此。同样的道理，CLR / JVM 这样的平台一定需要一个“强制的自动内存管理”（不一定是“非确定的内存管理”），因为它们的设计目标里面包含了对运行时 type safety 的保证——而一旦允许程序自己动手艹内存，这种保证就是在扯淡。显然，总会有程序以为自己可以正确的艹内存，但实际上却并非如此。严格来说，只有在操作系统内部，才是真正意义上“手动管理资源”的……吗？其实也不尽然。总会有操作系统觉得自己能管好一切资源，但实际上却并非如此。这也是各类“管理操作系统”的虚拟化技术出现的原因，或者至少是原因之一。

至于 deterministic vs indeterministic 这个问题，这方面的共识倒是比较多。总体上来说 indeterministic 会有更多的坑，强制应用 indeterministic 策略的话就会有更多。
finalizer 基本上是 GC 的阴暗面，里面的坑说实话不见得比 C++ 的析构函数少。finalizer 是不提供任何保证的，包括不保证它最终真的会被运行……如果只涉及本地资源的话还可以指望操作系统，但如果里面封装了非本地的资源句柄，good luck……

C# 好歹还是意识到了这个问题，所以有了 IDisposable。不过有了手动管理之后随之而来的就是资源所有权的问题……有些比较神奇的坑，比如说用一个 Stream 构造一个 BinaryReader 的话这个 BinaryReader 会默认自己取得了这个 Stream 的所有权，于是如果你 Dispose 了这个 BinaryReader，Stream 也跟着 BOOM 了。（新版的运行时给 BinaryReader 增加了一个构造函数重载来解决这个问题。）

为什么一个对象可以死了又活……

这个问题完全莫名其妙嘛，直到现在也还是不能理解这背后的设计逻辑。之前提到过一个，就是可能在执行异步的析构操作时需要保留状态。但从设计上讲这个问题不是不可避免的。还是觉得这样的设计很可能就是基于“万一有人在 finalizer 里面做奇怪的事情，比如说把这个对象注册到一个全局对象上去怎么办？”——“那就干脆让这个对象复活吧”

明明应该禁止这样的事情发生才对吧……（掀桌）

最后说个跑题的，话说我觉得构造函数这个东西应该要跟其他函数一样可以有个名字。这样的话就可以写类似这样的语法：

```
xml_document a = new xml_document::from_string(string);
xml_document b = new xml_document::from_file(filename);
```

话说所谓的 Factory Pattern 其实就是为了要给构造函数起个名字吧……

### 2015-08-02 02:40 幻の上帝

@Tippisum 自动和手动的界限是相对的，不会是铁板一块。

扔给操作系统算是一种策略，代价就是进程生存期内的确定性泄漏。手动的灵活性体现在用户有权决定是否采用这种策略。（当然这种策略一般很糟糕，这个另当别论。）

更自动的东西，就不方便有那么多可能性了。不过，损失可能性是不是值，还得看是否覆盖足够多的情况，或者说通用性。

愿意提供给下游用户多少控制，除了看设计者的心情，也得看实现是否距离边界足够远。

如果太高级又不够普遍，中间的资源管理层次多了效果也不好，自然会倾向于不提供底层资源管理接口的设计。

注意到不管是RAII还是典型地GC都也可以和new/delete共存，但不管是实现者还是用户付出的代价显然不一样。

因此GC这种“自动”注定更不靠谱——不止是习惯，而且不得不隐藏太多东西。

关于这里的一些资源的假设是建立在经典的hosted environment也就是一般意义上的操作系统的背景下的。

如果无视这些惯例，要刻意分配这些资源，也不是做不到。例如分配CPU时间，主要就是尽量确保实时性（不适合大部分系统的程度）以保证资源分配可预测的这个前提。

要做到底确实不得不深入指令集架构以下——例如任务切换可能是硬件提供的接口。然而，其中的大多数形式仍然来自于习惯。只是打破这种习惯的代价更大（需要重新制造硬件）。

涉及到时钟信号的资源的确是特殊的，它具有过期作废的物理属性，所以直接保存状态没有意义，没法抽象出保留给以后使用的资源。（我前几天刚好考虑过这个。）

形而上地要解决这里的特殊性也并不十分困难——把产生时钟信号的硬件作为资源，随用随取。说穿了关键就是虚拟化。

当然，一般不需要做到理想中的程度，非得放着硬件不用太傻了，除非原始需求指定了自己制造体系结构（自己造硬件或者模拟器/虚拟机之类的东西）。即便如此尽量抽象在实现上是不必要的，开销太大。

话说回来，最特殊的不过如此，其它资源的抽象就容易多了。

所以结论是“强制的自动管理”基本上还是人为约定，更多地仍然是习惯和现实需求的因素。考虑重要性，是这些因素的结果。

关于强制策略还是得提一下，根本问题还是在于需求。

不对面向的最终用户做假设的情况下，如果需求包含多任务/多用户，又不能预测执行任务的程序的可靠性，那么自然只能采取保守策略。

拿CPU时间分配来说，公平调度永远都不会是最终目的（更实际的一个目的是让用户程序合理地分配到CPU时间），而只是在上述环境下的实现策略。

一旦需求变化允许上面的假设不成立，这类强制策略也就不总是必要了。只是通用的系统不太容易有这些情况，所以一般就偷懒不考虑“优化”了。

退一步讲，在这类环境下，直接写死强制规则也不算是理想的解决方案。

这样就出现了权限的需要。其实更彻底的设计是权能(capabilities)，而且不应该限制到只能由操作系统来保证强制策略的实施。不过这是另外的问题了——本质上过于依赖特定底层机制就是上层安全体系的缺陷，虚拟化在此无能为力，即便实现了变通也经常不划算（这也是许多任务中为什么轻量级容器更受欢迎的主要理由）。

关于语言提供什么抽象的问题，我认为主要的坑都踩过一遍了，不应该继续拘泥在会普遍造成失败的套路上。所以我对一些不开窍的“新”设计相当不满。

构造函数没有名字当然是故意的。对静态语言来说，没有名字更偷懒一点，有了名字也基本就是个糖（且这类特例规则基本没卵用），所以直接选用让用户少写代码顺便简化语言规则的设计也容易理解。

对于C++来说，使用了这个设计之后也有其它语言特性和惯用法跟进，比如说构造函数往外抛异常catch不掉，能让用户能静态地保证不存在初始化一半程序继续往下跑的逗比状况。

而factory这样写，如果不是特别显著的最佳实践（比如make_xxx绕过构造函数不推断模板参数的限制），我会怀疑接口的设计者是不是想得太少了。

所以我自己基本不会设计这种API（即使构造函数太挤了要用helper也会放在类外面省得罗嗦），基本也不会遇到正经的C++ API使用这种风格。（Java之类不得不这样做的习惯另说。）

只是要名字清楚的话，注释和宏往往还更清楚一点……可以很容易地和上面怀疑误用的语义噪音划清界限。

### 2015-08-02 11:21 Tippisum

说穿了还是设计思路的问题。

C/C++运行时不介意你程序把它给艹了，反正UB糊脸你看着办，剩下的事情一概扔给OS。（其实在大部分时候CRT基本也可以算OS半个组成部分了）

相比之下CLR/JVM显然有着不能允许用户程序把运行时给艹了这样的需求。这就是问题所在。

OS有硬件撑腰，给进程提供的一切资源本质上都是虚拟化的，就算进程乱搞，在OS看来也是不疼不痒。但运行时没有足够的能力来这么弄，就只能艹资源抽象了。

悲剧的根源是运行时也把自己当操作系统。所以程序其实运行在多重的资源抽象上。何况OS可以假装让用户进程“手动管理”资源而不至于出什么大乱子，运行时往往做不到，所以自然就更疼了。

归根结底，虽然强制策略有时候不可避免，但能集中到一个层次上（比如OS）总比叠着好几层要好。能有硬件虚拟化支持的，总比纯粹靠艹抽象语义要好，这应该是没有疑问的。所以说OS层次的内存管理（包括GC）我也认为是一个正确的方向。

不过从实现的现实性来考虑，增加抽象层次总是比把现有抽象推倒重来要更容易。尽管现有的OS设计上很多时候不尽如人意，但相比于直接改变OS内核，显然目前更加主流的想法仍然是继续增加层次，往上走是各种应用虚拟机，往下走是各种硬件虚拟机……

而且现在计算机的计算能力确实过于强大以至于很多人已经习惯于在资源管理的问题上偷懒了。计算能力大部分时候过剩的PC平台姑且不论，实际上资源并没有那么富余的移动平台上，偷懒和乱搞的风气也很明显。

### 2015-08-03 01:08 Tippisum

关于 CPU 时间这个资源，我和你的看法有点不一样。

CPU 时间这个东西没有你想的那么特殊。时间特性本来就是资源的属性之一。说起来，本来也就没多少资源是可以“保存”的。网络带宽可以保存么？打印机？

或者干脆这样想，就算是内存这种最经典意义也最广泛讨论的资源，它可以保存么？

其实答案也是“不可以”。

假设系统的物理内存大小为 m，某一时刻 t 的时候，系统内所有进程请求的内存总和为 M。如果 M > m，则一定有 M - m 这么多的内存被换出。

* 这个事实不会因为 t 之前任何时刻系统的内存使用状况和内存分配策略而有任何改变。 *

* 即使 t 时刻之前整个系统没有任何内存请求，这些“过去的内存”也无论如何不可能被“保存”下来满足现在的资源请求。在 t 时刻之前，系统究竟是保持这些内存的空闲状态还是拿它们去干任何它想干的事情（例如用作硬盘的缓存，大部分现代的 OS 都会这么利用空闲的内存）并不会给这个状况带来什么区别。*

所以两者的待遇差别并不是物理属性导致的。

要说的话，就是因为 CPU 相比于内存而言是优先级更高，也更加敏感的资源。

还有一点，“内存”这个概念在不同的层次上指的其实是不同的东西。而真正珍贵的那种，也就是我们通常说的“物理内存”，* 实际上也是由操作系统强制的实施自动管理的。 *

在现代 OS 的视角看来，应用程序可以“手动管理”的那种“内存”实际上真的是一种相当贱的资源。因为那东西 * 只不过是由操作系统控制的虚拟内存罢了 *。就算程序真的申请内存从不释放，它实际能泄露的东西也不是内存，只不过是硬盘上的分页文件罢了。移动设备姑且不论，对于大多数桌面 PC 而言，分页文件这种资源的价值大约不会是一个很大的数字。

所以归根结底，还是 CPU 资源更敏感，而且更不好虚拟化——除非发生的非常频繁，否则进程一般很难察觉到自己的页面被操作系统强制的换出。但正在执行中的线程被抢占这个事情却有着实实在在的可见效应——同步、竞争、死锁……无数的麻烦事都和这个有关。

在一个通用的现代多用户、多任务系统（也就是在绝大多数时候我们所讨论的环境）下，如果一个资源，操作系统觉得它可以交给用户进程自己手动管理，那么有一个潜台词大概就是，用户进程其实也可以不管理，最多也就是把自己搞崩溃，或者把它权限所及的范围内折腾的一团糟。底线是这东西肯定不会也不能影响到整个系统的稳定运行。如果放任一个资源由用户进程乱搞有可能会把系统跑飞，那么操作系统一定会强制的接管它，最终暴露给用户进程的是一个被虚拟化的资源，抑或是某种抽象的接口（句柄 / 描述符 / etc.）

所以这也就是我之前说的，“如果安全性是一个 Requirement，那么自动资源管理一定会被强制的应用，无论这是以何种形式或者在何种层次上”。

当然，操作系统的资源管理和资源抽象既不一定合理，也不见得完善。所以才会有各种硬件虚拟机的出现——在操作系统的下面再加一个层次，替可能泄露的抽象兜底。另一方面，大多数时候 OS 它只关心用户进程不会把整个系统跑飞，至于程序会不会把自己跑飞这个事情才懒得操心。那些基于 GC 的“保姆型”运行时大约也就是基于这样的原因而开发出来，当然最终这些运行时反过来变成大爷把用户程序都按死在自己的条条框框里或者干脆假装自己也是个操作系统，这就是另一说了。

### 2015-10-14 10:39 幻の上帝

@Tippisum

对C和C++，UB糊脸可以说是一种几乎不得不使用的设计策略，原因：

1.划不来。

不UB，那么可以用运行时填坑，如JVM或者CLR。

但从语言实现的现状以及面对的应用领域来说不现实，可以预见不会被主要用户接受。

何况JVM和CLR这坨东西就算没有兼容性的坑，自身的工作量也不小（比如提供可被各方接受的元数据规范之类）。

2.做不到。

不UB，还可以通过做完全静态检查来完全避免。但对于已经有这么多洞的语言……且不说怎么改语言规范和考虑代码的兼容性——特性之间的兼容性就很可能有问题。

搞不好就是投入一坨资源发现实现进退两难，比export还糟糕。想想用户能乐见这样么——编译器：“臣妾做不到啊！”

要是重新设计另说，不过工业界在这方面的实践经验太少，还非常初级。

即便如此，有些地方不开洞不行。比如Rust不得不有unsafe……和C#这种（搞互操作性和性能优化目的的）少量用户才需要见到的情形还不一样，rc和链表什么的就别指望回避了——至少间接必须用到。

虽然作为不让GC背锅的代价嘛大概也可以接受……但考虑语言设计自身的复杂性、维护风险和成本，在演进上搞不好还不如允许UB这种简单直接粗暴的方式更优雅。
话说Rust的编译性能似乎非常呵呵的样子……让C艹用户以外的其他用户大部分忍耐，在这里都有难度。

还有一点要提的是C和C++明确允许freestanding implementation也就是不存在OS的情形——这在一些嵌入式设备以及写OS内核本身都有用到。

所以其实可以明确比“扔给OS”更干脆：敢不鸟OS？直接艹体系结构吧。是不是足够够用的虚拟化，看脸。（NDS没有MMU，RAM不够用没法实现有效的虚存就疼得要死……撸DSLinux那群人脑洞也是。）

如果抽象掉体系结构的这种managed语言明确允许freestanding impl，那时候语言运行时就在层次上直接取代OS的地位了。这倒未尝不可，只不过就没啥可用的东西。

像Cosmos什么的研究项目一坨，Inferno/Limbo也没啥存在感……

当然这些系统实现方案有明显劣于C/C++这样native的实现的技术因素，不过也没法忽视系统开发者和硬件厂商固守C传统的思想包袱的影响。

讽刺的是，大量最终用户却能（被迫）接受Android这样在应用上这里劣势更明显的方案……

### 2015-10-14 12:02 幻の上帝

@Tippisum.

来点脑洞吧……

关于 CPU 时间，表面的理想情况下确实不该那么特殊。

不过反思一下现实，会发现基本上所有解决方案的整个以通用性为主要目的的体系的设计都放弃了这种不特殊性而被容忍——恐怕这就是最特殊的地方。

现代通用系统的设计，普遍追求多任务的可用性，实时性退居其后——因为需要响应主要外界交互者首要的是人，而人对实时性是相对不敏感的，时间片轮转这样能换取明显更大灵活性（比如仅仅是逻辑上的多用户功能就能提供一些额外的容易伸缩部署的特性，更不用说虚拟化了）的替代策略成为主流也不奇怪。

换句话说，客观因素在于真正需要部署在特定物理机器和“硬实时”的问题现实占绝对少数。这导致通用领域的这种妥协的顺理成章。

（顺便，对这类不需要严格实时性的任务和大部分非专业用户来说，时间片这种概念是完全透明的——开发者不必要让用户了解它的存在，就能实现期望的系统行为；而相对地，GC造成的响应失败很容易超出了上面这些用户的预料。这也是默认使用GC这种策略欠扁的客观原因之一……）
反过来，要真能不妥协地做到这种理想情况，该如何设计和实现整个系统呢？

对于一个足够简单的系统来讲可能并不麻烦，但对于适应现代从业人员分工的要求上，应对需求变化的伸缩性和对环境适应性的限制极有可能风险过大而放弃采用，或者导致根本无法实现。

即便只考虑技术因素也有困难。注意在物理意义上现在并没有发现可靠的机制能用于保证这种抽象的整体性和完备性。保证用户能预期这些抽象的性质，还是得人为在每个层次上加上去的。

说实话我真不知道现在的工艺是不是能整得出来体系结构/微架构和以下足够多的每个层次上都能提供足够时间信息的硬件——似乎是不能指望的，但姑且先假设可行好了。那么就说上层的可行性。

描述开发者可用的接口主要形式是API——这要靠足够高级的语言来描述。那么这种语言该怎么设计来支持这样的需求呢？

如果不考虑时间，只考虑依赖空间的资源的描述，那么指令式语言里已经有相当完善的prior art——用确定的抽象表示特定时刻的状态并直接映射到存储上。

（这种静态刻画的抽象是如此自然和普遍，以至于大多数未经过程序语言理论专业训练的用户都忘了计算的本质，比如常常忽视表达式副作用在何种情形下有必要考虑而搞不清volatile是干什么用的，混淆volatile和atomic诸如此类……）

对于严肃的语言设计，考虑到规范实现的conformance等需要，确定具体程序含义这种用户需求是不够用的，而是得在语言规则确定和语言构造的语义。理想情况下这得有一整套形式化方法得到形式语义的结果，不过因为现实的困难（足够了解需求的专业人士不够用，成本太大，结果读起来太晦涩而且不便于实现等等），大部分语言做出了妥协。

像ISO C和ISO C++这样的主流（指令式为基础）的语言，使用的是操作语义这种形式语义方法的缩水版。

具体来说，做法是约定一个包含若干状态的抽象机，然后指定程序的行为是发生在这个抽象机上的操作，间接但相对清晰、（对规范实现来说）足够严格地表现出语义。

应该很容易注意到，这里的抽象提到了能够（通过memory model的规则）直接映射到存储空间上的“状态”，却压根就没提到“时间”。或者说，这样的语言，先天就缺乏把“时间”作为资源描述的能力。

这样，要把“时间”作为资源，和其它不直接就是存储的更高抽象程度的“高级”资源一样，得借由存储这个间接的手段。

对于一般的资源，C++也就是这样做的：约定具有表示存储的生存期的“对象”（这种抽象来自于C），然后加上可选的非平凡（non-trivial）的确定性析构来表示销毁资源的动作。

（题外话，我认为“对象”来加析构函数这种设计是不科学的。允许确定性析构是比存储本身更一般的特性。如果不要求对象才能有析构，而是引入其它更一般的抽象，就不用纠结scope guard占多少大小和allocator的空基类优化什么乱七八糟的开洞破事了。）

时间作为资源来说的意义，首先是预留可以执行计算和副作用的确定的可用性。换句话说，必须在某个时刻之前才有意义，过期作废。

这里就有个问题：该保存什么样的状态来描述“时间”这种资源？恐怕在语言内部找不出合理的有意义的设计——因为存储作为资源的前提就是某个确定却不和具体时刻关联的状态，而不是约束一段时间的状态，更不可能蕴含“特定时刻前的所有历史”。这样过期作废这个性质根本没法表现。
还有个问题：是什么机制有效地分配和回收时间，同时能保证资源可用性的？注意计量时间时执行状态更新就极有可能要不可逆地消耗时间，这个意义上，即便存在描述确定时刻的语言机制，就不可能可靠地知道该有多少资源被释放——更没法讨论回收的时机了。这倒是和GC只能（延迟）回收存储站在了对立面上。

退一步讲，假定某段程序在未来的执行已经被确定地、可靠地预言（“分配到资源”“不考虑回收”），且不考虑抽象这样的资源有什么用，怎么抽象上仍然有很大问题。要在这个基础上描述资源，需要编码状态（保存时刻）和策略，并且基本上必须得包含有选择地丢弃信息的策略（因为这里的存储——“对象”——是确定有限大小的，而一个现实程序的一段历史，形式化以后通常要么是无限的，要么是不确定地有限的）。一个通用的语言该如何知道用户程序的哪些历史是可以随便丢弃而设计成不影响语义的？除非限制涉及资源管理的语言机制的可用性，用另一套使用不同语义的语言子集来描述，根本无解。然而这样就丧失资源管理的普遍性了。

这并不是说，其它形式的语言规则也完全不能够刻画时间资源的意义。只是要描述清楚已经被确定的（即便包含可数无限状态）历史本身并不困难，但如何设计得可用以及明确语言实现该做什么是个很麻烦的问题。

去除可变状态——使任意子程序求值的历史可被预测的纯函数式风格，在理论和一定程度的实践上已经被证明是可行的了。但这是自废武功。因为描述状态的变化这种副作用（比如I/O操作）本来就是个普遍需求；没有副作用的程序和一个映射输入和输出的映射表无异，程序的“运行”根本就没什么用——根本就没描述清楚实现实际做的事——“怎么”计算，而不仅仅是计算“什么”。

所谓纯FP语言做到底也就只能把纯FP和非纯FP的部分划清界限而已，如Haskell这样。在表面上消灭了所有的副作用导致整体上看起来“简单”，实际却并不能推论出任意部分都满足整体约束性质这个结论，而且实现起来还破事一堆，拿来满足通用需求实在说不上是该褒扬推广的手段。而且，这个意义下时间这种资源不妨说是直接被架空了……根本是本末倒置嘛。

在不能可靠设计实现，而又没有足够动力去得到是否能够得到这种实现满足现有需求的确定回答时，把时间统括在内的这种资源抽象的普遍性是自发破缺的，只在自然语言的范畴内可行，并没有对机器解决问题有卵用。

所以，把“时间”特殊处理，也没什么不自然了。

偶尔发现一些其它观点之后，觉得这里需要老调重弹（结论没有新意）一下。

（还有一个问题是这层楼里我发现零碎地涉及到更一般的几套观点或者说“哲学”，有必要系统性地单独论述，不过这个坑现在填不完。）

我试着从两个方面重新审视资源管理机制的整体设计。这两部分之前似乎一直混着说，不过现在我认为有明确分别阐述的必要。

其一个是架构设计的意义，仍然是考虑（主观上不能随意回避的）需求。

OS提供虚拟资源的主要理由，并不是有硬件撑腰，而是找不到其它靠谱的办法来提供对所有用户（开发者）可用的一致抽象，所以需要分层带来的可伸缩性。反过来因为性能等原因才迫使硬件提供特定的虚拟化实现。这里预设的“分层”策略是对现实问题的一种合理简化，因为把用户需要使用的接口的目的以不同层次来概括是合理有效的。

反过来，简化并不是指最后尽量要集中在一层——这本身不是目的。我之前说过过OS层次管理内存是一个应该考虑的方向，这其实只说了一半；潜台词是有些“管理”当然注定不可能在底层处理好。于是分层架构和对应每一层的抽象（“多重”）会自然地长期存在，只不过机制设计上会有所侧重而已。

因为提供不同粒度管理机制的客观需求这点没法现实地消灭，所以集中在一个层次上也不能指望好了。注意，这并不只是资源管理机制的实现“做不到”的问题。

当然，不管怎么说，JVM这种不按操作系统方式部署却容易阻碍整个系统运行的使用GC的不透明应用虚拟机仍然是鸡肋（“运行时把自己当操作系统”）——没有疑问的是这点。

关于适应资源的性质也同样是虚拟资源的理由之一。

虽然抽象上存在不同种类的资源，但底层实现无非那么一些——比如互斥体这种高级的资源，考虑CPU调度以后就可以分解为具体实现的CPU时间和存储空间而已。显而易见就不是所有资源都得物理地实现（这也不现实），自然会出现一定层次上的虚拟了。

当然，这些资源之所以称为“高级”，正因为它不是底层资源的随意组合，而是通过限制可能的组合方式提供了接口不变量之类的其它抽象的语义，不过这是另外的话题了。
内存空间作为不那么高级的资源，

另一方面是资源抽象自身的目的（用户的主观需求）。这个和整体而言需求的关联看起来不那么明显，但是某种意义上发挥的影响更加显著。

我之前一直批判一些资源抽象的默认惯用做法脱离需求，但没有系统地分析成因。
毫无疑问，一个主要动机是“简化”。传统Lisp风格的抽象认为一个可管理的状态抽象（“对象”）默认具有不确定的可用性(indefinite extent)，另一些具有在运行时确定的生存期(runtime extent)。

默认使用GC的策略，在描述程序含义的模型上，基本上都得依赖于此类设计。

我要强调的是，只有后者才是具有资源语义的“对象”的生存期(lifetime)。而前者是用户一厢情愿臆造出来的抽象。理由之前也说了，deterministic的性质是物理上不可避免的，引入indefinite之后总需要有一套东西把它消灭掉然后确定出runtime。关键在于，这么搞其实过于复杂了。

## 补充注记 2016-04-28 12:02

补记一些略相关的notes。


之前说过只要有reference value type，pass by value可以理论上全盘代替pass by reference，反过来不行。不过这里隐含了两点：

1.call by value中，pass by value可能的side effect独立于callee之外，由caller负责。这也就是为什么C++中典型的unifiying copy constructor可以直接noexcept的原因。因为param initialization在进入activation record frame之前，发生的side effect sequenced before任何call内部的side effect，内部可以提供nothrow exception guarantee而不用管可能会throw的copy constructor call。calling convention具有相似的责任归属问题（实现上由caller还是callee来在退出call时撤销activation record frame），但那完全不用涉及到side effects，相对比较简单，可以单独指定。这里有一个差异是caller指定在不经变换时可能会冗余清理frame的目标代码，但却能因为保留更多的caller信息而有更多的效果，例如va_args在x86上用__cdecl实现但不能用__stdcall实现；考虑可以使用inter procedural optimization，通用的默认情况使用__cdecl是应当被接受的。相似的作用在上层的paramater passing的side effect上似乎没有体现，可能存在优化空间（但看样子必然引入语言上的复杂性）。

2.非引用的初始化被视为special forms。这样才能“只提供reference”。只是这点超出了pass by reference的范围，所以是个附加的假设。

lambda capture有类似的问题，可能更清楚点。很少语言提供capture by value，除了因为lambda calculus的原始形式外，另一个理由是capture by value可以通过capture by reference之后显式复制一个不同的value来得到。然而这样做有一些问题：

1.和之前说的一样，parameter initialization无法避免side effect的从属问题。要指定清楚自然也是可以的，但太麻烦了，而且capture和pass都应该一致——后者和lambda calculus的原始形式不一样，不指出initialization的side effect而放任其隐式发生可能很容易发生一些比parameter passing更难看清的问题，比如performance regression。

2.真需要value copy的情况其实不少。这时就有效地迫使用户在lambda abstraction内对captured variable进行手动的renaming，除非提供其它复杂的scoping机制。实际上naming（包括考虑新的不重复的name、判断是否适合在这个位置用这个name、判断是否有更好的name和猜测其他人是否容易理解这个name）对人来讲往往是比较头疼的，不只是captured variable，也包括一般的local scope以及作为parameter的（虽然现在gcc仍然常年开`[-Wshadow]`）……再考虑一下C#的不爽的玩意儿……

3.因为表面上要求simplicity实际上就是偷懒的关系，value copy（名词，副本）在一些语言和实现上会真的去copying（动词，复制），不但在概念上容易混淆call/pass/copy，也可能带来一些性能问题（因为在这里的情形下的copying不是一定那么容易optimized away）。对比C++，copy initialization提供copy和move的不同选项，同时要求考虑copy elision，就实际上要求实现和用户都重视这些问题。（虽然不管verbosity of wording/simplicity of design/teachability上还是都很糟烂，而且现在也没强制要求copy elision反而多出来unspecified behavior的坑。）

4.在没有GC的支持下，capture by reference本身具有显著的危险性（过于容易造成dangling reference）。反过来，如果默认只有capture by reference，就迫使语言加入GC了（终于点题了……）——否则这种基础设施普遍情形下就不可能单独能用（简单情况下，至少配合一个value copy）。这倒过来损害了对resource这个abstraction的expresiveness。

所以合理的设计中capture by value中是不能被单独取代的。

